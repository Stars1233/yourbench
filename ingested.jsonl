{"document_id": "5a12620c-054a-4fef-9d4f-95f3c2f9bc35", "document_text": "# Using OpenAI Compatible Models\n\nYourBench supports using any OpenAI-compatible model by configuring the `base_url` parameter in your YAML configuration.\n\n## Configuration\n\nAdd your OpenAI-compatible model to the `model_list` section of your configuration YAML:\n\n```yaml\nmodel_list:\n  - model_name: gpt-4o\n    base_url: \"https://api.openai.com/v1\"  # Default OpenAI API URL\n    api_key: $OPENAI_API_KEY\n    max_concurrent_requests: 10\n\n  # Example for an Anthropic Server\n  - model_name: claude-3-7-sonnet-20250219\n    provider: null\n    base_url: \"https://api.anthropic.com/v1/\"  # Replace with your API endpoint\n    api_key: $ANTHROPIC_API_KEY\n    max_concurrent_requests: 5\n```\n\n## Environment Variables\n\nSet the required API keys as environment variables. For example:\n\n```bash\nexport OPENAI_API_KEY=your_openai_api_key\nexport ANTHROPIC_API_KEY=your_anthropic_api_key\n```\n\n## Model Roles\n\nAssign your models to specific pipeline roles:\n\n```yaml\nmodel_roles:\n  ingestion:\n    - gpt-4o  # For vision-supported tasks\n  summarization:\n    - claude-3-7-sonnet-20250219\n  chunking:\n    - intfloat/multilingual-e5-large-instruct\n  single_shot_question_generation:\n    - gpt-4o\n  # using multiple models for question generation\n  multi_hop_question_generation:\n    - claude-3-7-sonnet-20250219\n    - gpt-4o\n```", "document_filename": "USING_OPENAI_COMPATIBLE_MODELS.md", "document_metadata": {"file_size": 1308}}
{"document_id": "b8f7a4d0-c4a8-4e26-a141-b6b4605ff6fa", "document_text": "# FAQ: Frequently Asked Questions about YourBench\n\nWelcome to the **YourBench** FAQ! This document aims to answer common questions about what YourBench is, how it works, and how you can use it to generate dynamic, document-grounded evaluation sets for Large Language Models. Below, you'll find practical information on installation, configuration, usage, and more.\n\n---\n\n## 1. What Is YourBench?\n\n**YourBench** is an open-source framework designed to **generate new, domain-specific or up-to-date benchmarks** for evaluating Large Language Models (LLMs). Instead of relying on static (and often outdated or contaminated) benchmarks, YourBench takes _your_ custom documents—such as company reports, specialized academic texts, or newly published web content—and **automatically creates fresh question-answer pairs** to test models on relevant content. By doing so, it helps you:\n\n- Avoid contamination with older, widely used datasets.\n- Generate relevant evaluations for specialized or emerging topics.\n- Automatically produce large volumes of questions with minimal human labor or cost.\n\nYourBench also supports advanced pipeline stages like summarization, chunking, multi-hop question generation, citation filtering, and more, as detailed below.\n\n---\n\n## 2. What Does the General Pipeline Look Like?\n\nYourBench’s pipeline typically consists of these stages (in a default order):\n\n1. **Ingestion**  \n   Converts raw documents (PDF, MD, HTML, DOCX, etc.) into a normalized Markdown/text format.\n\n2. **Upload/Save**  \n   Optionally packages ingested documents into a Hugging Face Dataset and pushes it to the Hub (or saves locally).\n   Each stage can save its intermediate output either locally or to the Hugging Face Hub (or both), depending on your hf_configuration.\n   - Local saving is controlled by `local_saving` and `local_dataset_dir`.\n   - Remote saving (Hub) is controlled by `push_to_hub` and credentials (token + dataset name). This ensures that downstream stages can reliably load subsets across runs.\n\n3. **Summarization**  \n   Creates short summaries of each document or chunk to provide global context.\n\n4. **Chunking**  \n   Splits documents into manageable single-hop segments (e.g., by tokens or semantically) and optionally creates multi-hop groupings.\n\n5. **Single-Shot Question Generation**  \n   Generates questions from individual chunk(s) plus global summary. (These are usually simpler, fact based, or straightforward questions)\n\n6. **Multi-Hop Question Generation**  \n   Generates more complex, integrative questions by referencing multiple chunks. (These are usually more complex questions). Both single shot and multi-hop generate questions, however, the nature of the questions generated by both is very different!\n\n8. **LightEval**  \n   Assembles those questions into a final “evaluation dataset” that includes question text, ground truth, citations, and relevant chunk text.\n\n9. **Citation Score Filtering**  \n   Performs fuzzy string matching to gauge how well each question’s citations match the source text, optionally filtering low-scoring items.\n\nYou can control which stages run by toggling them in your config (e.g., `pipeline.ingestion.run: true` or `false`).\n\n---\n\n## 3. How Do I Install and Set Up YourBench?\n\n1. **Clone the Repository**  \n   ```bash\n   git clone https://github.com/huggingface/yourbench.git\n   cd yourbench\n   ```\n\n2. **Install Dependencies**  \n   We recommend using a virtual environment. Then install with:\n   ```bash\n   pip install -r requirements.txt\n   ```\n   (If you plan to do semantic chunking or advanced tasks, you’ll need PyTorch, Transformers, etc. as indicated in the repository docs.)\n\n3. **Configure Your Environment Variables**  \n   If you plan to push to the Hugging Face Hub, set your Hugging Face token in an `.env` file or as an environment variable (`HF_TOKEN`).\n\n4. **Prepare a Configuration File**  \n   You can start from `example/configs/simple_example.yaml` or `example/configs/advanced_example.yaml`. Adjust it to point at your source documents and specify the models you want to use.\n\n---\n\n## 4. How Do I Run the Pipeline?\n\n1. **Create or Edit a Config File**  \n   - Check out `example/configs/simple_example.yaml` (the minimal version).\n   - Update `source_documents_dir` to the folder containing your raw data.\n   - Optionally specify which model(s) you want for each pipeline stage in `model_list` and `model_roles`.\n\n2. **Call the YourBench CLI**  \n   From within your repository directory, run:\n   ```bash\n   python -m yourbench.main run --config path/to/your_config.yaml\n   ```\n   You can enable debug logging with this flag.\n   ```bash\n   python -m yourbench.main run --config path/to/your_config.yaml --debug\n   ```\n\n3. **View the Outputs**  \n   - By default, intermediate datasets are stored on Hugging Face Hub (if configured) and/or locally, named according to your `hf_configuration`.\n   - Logs (errors, pipeline progress, etc.) are written to the `logs/` folder.\n\n---\n\n## 5. How Should I Structure My Documents?\n\nYourBench is flexible. Typically:\n\n- **Ingestion** converts each raw file (PDF, MD, HTML, DOCX, etc.) into a standardized Markdown.  \n- If your documents are already in plain text or Markdown, just place them in a folder and point `ingestion.source_documents_dir` there.\n\nMulti-document ingestion is handled automatically: each file becomes a separate “document” entry in the resulting dataset.\n\n---\n\n## 6. Can I Use Multiple Models in the Pipeline?\n\nAbsolutely. In your config’s `model_list`, define multiple models. For example:\n```yaml\nmodel_list:\n  - model_name: gpt-4.1\n    base_url: https://api.openai.com/v1\n    api_key: $OPENAI_API_KEY\n  - model_name: Qwen/Qwen3-30B-A3B\n    provider: fireworks-ai\n```\nThen in `model_roles`, assign which model(s) perform each stage:\n```yaml\nmodel_roles:\n  ingestion:\n    - Qwen/Qwen3-30B-A3B\n  summarization:\n    - gpt-4.1\n    - Qwen/Qwen3-30B-A3B\n  single_shot_question_generation:\n    - gpt-4.1\n```\nYourBench will run inference calls in parallel for each model assigned.\n\n---\n\n## 7. How Do I Generate Multi-Hop Questions?\n\n- Ensure the pipeline’s `multi_hop_question_generation` stage is set to `run: true`.\n- Make sure your chunking stage is also on and includes `multihop_chunks`, or define multi-hop chunking parameters in your config:\n  ```yaml\n  pipeline:\n    chunking:\n      chunking_configuration:\n        chunking_mode: semantic_chunking\n        h_min: 2\n        h_max: 5\n        num_multihops_factor: 5\n    multi_hop_question_generation:\n      run: true\n      # additional instructions or chunk sampling, etc.\n  ```\n- YourBench will then sample multi-chunk sets and call your chosen model(s) to produce questions requiring multiple pieces of context.\n\n---\n\n## 8. How Does Citation Filtering Work?\n\nAfter generating questions, YourBench can **verify if each question is grounded** in its source chunk(s) by fuzzy matching. The pipeline stage `citation_score_filtering`:\n\n1. Compares the alleged citations to the actual chunk text (and optionally the ground-truth answer).\n2. Computes a “citation_score” by measuring string overlap (using partial ratio from `thefuzz`).\n3. Lets you filter or rank questions by how strongly they’re anchored in the original text.\n\n---\n\n## 9. Can I Replicate Something Like MMLU with YourBench?\n\nYes! In the paper, we demonstrate replicating the style and relative difficulty of MMLU subsets:\n\n1. Collect a few relevant documents for each subject domain (e.g., a handful of Wikipedia articles).\n2. Run the pipeline to generate multiple-choice questions.\n3. Evaluate your LLMs on these newly generated sets.\n   \nThe results strongly correlated with the original MMLU in ranking models, but the newly generated questions are “harder” and are contamination-resistant. Just be sure to adapt your prompt instructions so that the question generation yields multiple-choice style Q&A.\n\n---\n\n## 10. Where Are My Final Questions Stored?\n\n- After single-shot or multi-hop generation, your “raw” question datasets appear under subset names like `single_shot_questions`, `multi_hop_questions`.\n- The pipeline’s `lighteval` stage merges them into a single dataset called `lighteval`, containing columns like `question`, `ground_truth_answer`, `citations`, and the associated chunk(s).\n- By default, these subsets are saved on the HF Hub (under your designated dataset name) and/or locally, depending on your config.\n\n---\n\n## 11. Do I Have to Push Everything to the Hugging Face Hub?\n\nNot necessarily. In your config’s `hf_configuration`, you can disable or enable pushing:\n\n```yaml\nhf_configuration:\n  local_saving: true              # Enables saving to disk\n  local_dataset_dir: ./results/datasets  # Where datasets are saved locally\n  push_to_hub: true               # Optional: also push each stage result to the Hub\n  concat_if_exist: false          # Whether to merge with existing datasets\n  # private: true                 # Whether Hub datasets should be private\n\n```\nYou can set `local_dataset_dir` (under `hf_configuration`) to a path and store your resulting datasets entirely locally — as long as `local_saving: true` is also set. Alternatively, you can enable both local saving and Hub pushing. The pipeline is flexible to your preference.\n\n---\n\n## 11b. How Are Intermediate Datasets Saved?\n\nEach pipeline stage saves its result using `custom_save_dataset()`. The behavior depends on both:\n- The config file, especially `hf_configuration.local_saving` and `local_dataset_dir`.\n- The per-stage logic, which calls:\n```yaml\nhf_settings = get_hf_settings(config)\ncustom_save_dataset(\n    dataset=dataset,\n    config=config,\n    subset=\"stage_name\",  # e.g., \"summarized\", \"chunked\"\n    save_local=hf_settings.local_saving,\n    push_to_hub=True,\n)\n```\n\nThis ensures datasets are:\n- Persisted between stages, even across different runs.\n- Reloadable by exact subset name (e.g., \"chunked\"), preventing missing subset errors.\n\n## 12. What If My Documents Are Very Large?\n\nFor large documents, the pipeline automatically:\n\n- Splits (chunking) by token-based thresholds or semantic boundaries.\n- Summarizes each chunk to keep context windows from overflowing your model’s max context length.\n- Optionally merges chunk-level summaries into a single short “document_summary.”\n\nBecause chunking is crucial for big inputs, carefully tune the chunking config (e.g., `l_max_tokens`, overlap, or semantic threshold) to ensure coverage without overloading your model.\n\n---\n\n## 13. What If My Model Has a Specific Context Window or Memory Constraint?\n\nAdjust the pipeline config to keep chunk sizes within that limit. For example:\n```yaml\npipeline:\n  summarization:\n    max_tokens: 16384\n  chunking:\n    chunking_configuration:\n      chunking_mode: fast_chunking\n      l_max_tokens: 128  # or 1024 or 4096, depending on your model\n      token_overlap: 128\n```\nThese parameters let you manage how aggressively we split large documents and how much overlap we maintain between splits.\n\n---\n\n## 14. Is YourBench Only for English Text?\n\nNo, the pipeline itself is language-agnostic. If your model supports a given language, YourBench can ingest and generate questions for that language. For chunking in semantic mode, ensure you select a suitable multilingual embedding model (e.g., `intfloat/multilingual-e5-large-instruct`) in the config.\n\n---\n\n## 15. How Do I Control the Cost or Limit Inference Calls?\n\n- **Subset your data** using the `chunk_sampling` config to generate fewer questions.  \n- **Reduce multi-model usage** if you only need a single model for question generation.  \n- **Use smaller language models** for some stages (like summarization or ingestion) while using larger ones only for question generation.  \n- **Lower the `multi_hop_question_generation.num_multihops_factor`** to limit the number of multi-chunk combos.\n\n---\n\n## 16. Where Can I Find Further Technical Details?\n\n- The [**Paper**](https://arxiv.org/abs/2504.01833) provides a conceptual overview, demonstration, and thorough validation results.\n- Each pipeline stage’s code is in `yourbench/pipeline/`.  \n- Utility modules (e.g., for inference concurrency, chunking, dataset management) are in `yourbench/utils/`.\n- The top-level CLI is in `yourbench/main.py`.\n\n---\n\n## 17. How Can I Contribute or Raise Issues?\n\nWe welcome feedback, feature requests, and bug reports! Feel free to:\n\n- Open an issue on our GitHub repository.\n- Submit a pull request if you have improvements or new features to propose.\n\n---\n\n## 18. Any Ethical Considerations?\n\nYourBench can automate large-scale question generation and potentially replace some annotation tasks, which raises labor considerations. Additionally, if your LLM is biased or inaccurate, those biases can propagate into the generated benchmarks. It’s crucial to:\n\n- Evaluate the outputs with human oversight.\n- Use filtering steps (e.g., citation_score_filtering) or human review to catch low-quality or biased content.\n- Be transparent about how these synthetic benchmarks are created.\n\n---\n\n## 19. What’s Next?\n\nConsider trying these advanced workflows:\n\n- **Creating Domain-Specific Benchmarks**: Provide proprietary or niche documents (e.g., medical guidelines, legal briefs) to assess your model’s real-world domain knowledge.\n- **Temporal Evaluations**: Use newly published documents (like the `Tempora-0325` set from the paper) to see if your model can handle post-training knowledge.\n- **Multi-hop Reasoning**: If your domain’s content requires integrative questions, ensure multi-hop chunk generation is enabled.\n\nHappy benchmarking, and we hope YourBench transforms how you generate and evaluate custom LLM benchmarks!\n\n---\n\n*If you have other questions, please open an Issue or check the repository’s README for the most up-to-date information.*", "document_filename": "FAQ.md", "document_metadata": {"file_size": 13813}}
{"document_id": "fff37b6b-9eac-40d8-a070-c385dca944c4", "document_text": "# YourBench Configuration Guide\n\nYourBench uses a flexible YAML-based configuration system built on Pydantic models. This guide covers all configuration options and usage patterns.\n\n## Table of Contents\n\n- [Overview](#overview)\n- [Configuration Structure](#configuration-structure)\n- [Core Components](#core-components)\n  - [Hugging Face Configuration](#hugging-face-configuration)\n  - [Model Configuration](#model-configuration)\n  - [Pipeline Configuration](#pipeline-configuration)\n- [Pipeline Stages](#pipeline-stages)\n  - [Ingestion](#ingestion)\n  - [Summarization](#summarization)\n  - [Chunking](#chunking)\n  - [Question Generation](#question-generation)\n  - [Question Rewriting](#question-rewriting)\n  - [LightEval](#lighteval)\n  - [Citation Score Filtering](#citation-score-filtering)\n- [Advanced Features](#advanced-features)\n  - [Environment Variables](#environment-variables)\n  - [Custom Prompts](#custom-prompts)\n  - [Model Role Assignment](#model-role-assignment)\n- [Configuration Examples](#configuration-examples)\n- [API Reference](#api-reference)\n\n## Overview\n\nYourBench configuration is structured around three main components:\n\n1. **Hugging Face Configuration** - Settings for dataset management and uploads\n2. **Model Configuration** - LLM provider settings and API credentials\n3. **Pipeline Configuration** - Stage-specific settings for the processing pipeline\n\n## Configuration Structure\n\nA typical YourBench configuration file follows this structure:\n\n```yaml\nhf_configuration:\n  # Hugging Face dataset settings\n\nmodel_list:\n  # List of model configurations\n\nmodel_roles:\n  # Optional: Assign specific models to pipeline stages\n\npipeline:\n  # Pipeline stage configurations\n```\n\n## Core Components\n\n### Hugging Face Configuration\n\nControls dataset naming, organization, and upload behavior.\n\n```yaml\nhf_configuration:\n  hf_dataset_name: my-dataset-name  # Default: random name\n  hf_organization: $HF_ORGANIZATION  # Environment variable\n  hf_token: $HF_TOKEN               # Environment variable\n  private: false                    # Dataset visibility\n  concat_if_exist: false           # Append to existing dataset\n  local_dataset_dir: data/saved_dataset  # Local save path\n  local_saving: true               # Save locally\n  upload_card: true                # Upload dataset card\n```\n\n**Key Features:**\n- Environment variable expansion with `$VARNAME` syntax\n- Automatic HF organization detection from token if `$HF_ORGANIZATION` is not set\n- Random dataset names generated if not specified\n\n### Model Configuration\n\nDefines LLM providers and connection settings.\n\n```yaml\nmodel_list:\n  - model_name: gpt-4o\n    base_url: https://api.openai.com/v1/\n    api_key: $OPENAI_API_KEY\n    max_concurrent_requests: 32\n    encoding_name: cl100k_base\n    provider: auto  # Automatically set if not specified\n    bill_to: optional-billing-id\n```\n\n**Supported Fields:**\n- `model_name`: Model identifier (required)\n- `base_url`: API endpoint (optional)\n- `api_key`: Authentication token (supports env vars)\n- `max_concurrent_requests`: Rate limiting (1-100)\n- `encoding_name`: Tokenizer encoding\n- `provider`: Provider type (auto-detected if not set)\n\n### Pipeline Configuration\n\nControls which stages run and their parameters.\n\n```yaml\npipeline:\n  ingestion:\n    run: true  # Automatically set when stage is present\n    # ... stage-specific config\n  \n  summarization:\n    # Presence implies run: true\n    # ... stage-specific config\n```\n\n## Pipeline Stages\n\n### Ingestion\n\nProcesses raw documents into structured markdown format.\n\n```yaml\npipeline:\n  ingestion:\n    source_documents_dir: path/to/documents\n    output_dir: path/to/output\n    upload_to_hub: true\n    llm_ingestion: false  # Use LLM for PDF extraction\n    pdf_dpi: 300         # PDF rendering quality (72-600)\n    pdf_llm_prompt: \"\"   # Custom prompt or file path\n    supported_file_extensions:\n      - .pdf\n      - .docx\n      - .html\n      - .md\n      # ... more extensions\n```\n\n**LLM-Powered PDF Extraction:**\nWhen `llm_ingestion: true`, PDFs are processed using an LLM for better extraction quality. You can customize the prompt by providing either:\n- Direct prompt text\n- Path to a prompt file (`.md`, `.txt`, `.prompt`)\n\n### Summarization\n\nCreates document summaries for better context understanding.\n\n```yaml\npipeline:\n  summarization:\n    max_tokens: 32768\n    token_overlap: 512\n    encoding_name: cl100k_base\n    summarization_user_prompt: \"\"  # Custom prompt or file path\n    combine_summaries_user_prompt: \"\"  # Custom prompt or file path\n```\n\n### Chunking\n\nSplits documents into manageable chunks for question generation.\n\n```yaml\npipeline:\n  chunking:\n    l_max_tokens: 8192      # Maximum chunk size\n    token_overlap: 512      # Overlap between chunks\n    encoding_name: cl100k_base\n    h_min: 2               # Minimum hop distance\n    h_max: 5               # Maximum hop distance\n    num_multihops_factor: 1  # Multi-hop generation factor\n```\n\n### Question Generation\n\nYourBench supports three question generation strategies:\n\n#### Single-Shot Question Generation\n\n```yaml\npipeline:\n  single_shot_question_generation:\n    question_mode: open-ended  # or \"multi-choice\"\n    additional_instructions: \"Focus on technical details\"\n    single_shot_system_prompt: path/to/prompt.md\n    single_shot_system_prompt_multi: path/to/multi_choice_prompt.md\n    single_shot_user_prompt: path/to/user_prompt.md\n```\n\n#### Multi-Hop Question Generation\n\n```yaml\npipeline:\n  multi_hop_question_generation:\n    question_mode: open-ended\n    additional_instructions: \"\"\n    multi_hop_system_prompt: path/to/prompt.md\n    multi_hop_system_prompt_multi: path/to/multi_choice_prompt.md\n    multi_hop_user_prompt: path/to/user_prompt.md\n```\n\n#### Cross-Document Question Generation\n\n```yaml\npipeline:\n  cross_document_question_generation:\n    question_mode: open-ended\n    max_combinations: 100\n    chunks_per_document: 1\n    num_docs_per_combination: [2, 5]  # [min, max]\n    random_seed: 42\n    # ... prompt configurations\n```\n\n### Question Rewriting\n\nImproves question naturalness and clarity.\n\n```yaml\npipeline:\n  question_rewriting:\n    additional_instructions: \"Make questions conversational\"\n    question_rewriting_system_prompt: path/to/prompt.md\n    question_rewriting_user_prompt: path/to/prompt.md\n```\n\n### LightEval\n\nPrepares data for evaluation.\n\n```yaml\npipeline:\n  prepare_lighteval:\n    run: true\n  \n  lighteval:\n    run: true\n```\n\n### Citation Score Filtering\n\nFilters questions based on citation quality metrics.\n\n```yaml\npipeline:\n  citation_score_filtering:\n    subset: prepared_lighteval\n    alpha: 0.7  # Weight for metric 1\n    beta: 0.3   # Weight for metric 2 (alpha + beta = 1.0)\n```\n\n## Advanced Features\n\n### Environment Variables\n\nYourBench automatically expands environment variables prefixed with `$`:\n\n```yaml\nhf_configuration:\n  hf_token: $HF_TOKEN\n  hf_organization: $HF_ORGANIZATION\n\nmodel_list:\n  - api_key: $OPENAI_API_KEY\n```\n\n**Special Behavior:**\n- If `$HF_ORGANIZATION` is not set, YourBench attempts to retrieve it from the Hugging Face token\n\n### Custom Prompts\n\nPrompts can be specified in three ways:\n\n1. **Built-in defaults** - Leave field empty\n2. **Inline text** - Provide prompt directly in YAML\n3. **File reference** - Path to `.md`, `.txt`, or `.prompt` file\n\n```yaml\n# Method 1: Use default\nsummarization_user_prompt: \"\"\n\n# Method 2: Inline\nsummarization_user_prompt: \"Summarize the key points...\"\n\n# Method 3: File reference\nsummarization_user_prompt: path/to/custom_prompt.md\n```\n\n**Package Resource Loading:**\nYourBench first attempts to load prompts from the installed package's `yourbench.prompts` directory, ensuring compatibility with pip-installed versions.\n\n### Model Role Assignment\n\nAssign specific models to pipeline stages:\n\n```yaml\nmodel_list:\n  - model_name: gpt-4o\n  - model_name: claude-3-opus\n\nmodel_roles:\n  ingestion: [gpt-4o]\n  summarization: [claude-3-opus]\n  question_generation: [gpt-4o]\n  # Unspecified stages use the first model\n```\n\n## Configuration Examples\n\n### Minimal Configuration\n\n```yaml\nhf_configuration:\n  hf_dataset_name: my-benchmark\n\nmodel_list:\n  - model_name: gpt-4o\n\npipeline:\n  ingestion:\n    source_documents_dir: data/documents\n  single_shot_question_generation:\n```\n\n### Advanced Configuration\n\n```yaml\nhf_configuration:\n  hf_dataset_name: advanced-benchmark\n  hf_organization: my-org\n  private: true\n  concat_if_exist: true\n\nmodel_list:\n  - model_name: gpt-4o\n    base_url: https://api.openai.com/v1/\n    api_key: $OPENAI_API_KEY\n    max_concurrent_requests: 50\n  \n  - model_name: claude-3-opus\n    base_url: https://api.anthropic.com/v1/\n    api_key: $ANTHROPIC_API_KEY\n\nmodel_roles:\n  ingestion: [gpt-4o]\n  summarization: [claude-3-opus]\n  question_generation: [gpt-4o]\n\npipeline:\n  ingestion:\n    source_documents_dir: data/raw\n    output_dir: data/processed\n    llm_ingestion: true\n    pdf_llm_prompt: prompts/pdf_extraction.md\n  \n  summarization:\n    max_tokens: 64000\n    summarization_user_prompt: prompts/custom_summary.md\n  \n  chunking:\n    l_max_tokens: 16384\n    h_min: 3\n    h_max: 7\n  \n  single_shot_question_generation:\n    question_mode: multi-choice\n    additional_instructions: \"Create challenging technical questions\"\n    single_shot_system_prompt_multi: prompts/mc_system.md\n  \n  multi_hop_question_generation:\n    question_mode: open-ended\n  \n  question_rewriting:\n    additional_instructions: \"Ensure questions are clear and unambiguous\"\n  \n  prepare_lighteval:\n  \n  citation_score_filtering:\n    alpha: 0.8\n    beta: 0.2\n```\n\n## API Reference\n\n### Loading Configuration\n\n```python\nfrom yourbench.utils.configuration_engine import YourbenchConfig\n\n# Load from YAML file\nconfig = YourbenchConfig.from_yaml(\"config.yaml\")\n\n# Create programmatically\nconfig = YourbenchConfig(\n    hf_configuration=HuggingFaceConfig(\n        hf_dataset_name=\"my-dataset\"\n    ),\n    model_list=[\n        ModelConfig(model_name=\"gpt-4o\")\n    ]\n)\n```\n\n### Accessing Configuration\n\n```python\n# Check if stage is enabled\nif config.is_stage_enabled(\"ingestion\"):\n    ingestion_config = config.pipeline_config.ingestion\n    \n# Get model for stage\nmodel_name = config.get_model_for_stage(\"summarization\")\n\n# Get enabled stages in execution order\nenabled_stages = config.pipeline_config.get_enabled_stages()\n```\n\n### Saving Configuration\n\n```python\n# Save to YAML file\nconfig.to_yaml(\"output_config.yaml\")\n\n# Get YAML string\nyaml_str = config.model_dump_yaml()\n```\n\n### Validation\n\nAll configuration is validated using Pydantic:\n\n- Type checking\n- Range validation (e.g., DPI between 72-600)\n- Constraint validation (e.g., alpha + beta = 1.0)\n- Path validation and auto-creation\n- Environment variable expansion\n\n## Best Practices\n\n1. **Use environment variables** for sensitive data like API keys\n2. **Start with minimal config** and add stages as needed\n3. **Leverage default prompts** before writing custom ones\n4. **Test with small datasets** before processing large corpora\n5. **Enable debug mode** for troubleshooting: `debug: true`\n6. **Use model roles** when different stages need different models\n7. **Version control** your configuration files\n8. **Document custom prompts** clearly for team collaboration\n\n## Troubleshooting\n\nCommon issues and solutions:\n\n- **Missing environment variables**: Export required vars or use `.env` file\n- **Path not found**: Use absolute paths or paths relative to config file\n- **Validation errors**: Check field types and constraints in error message\n- **Model assignment**: Ensure model names in `model_roles` match `model_list`\n- **Prompt loading**: Verify file paths and extensions for custom prompts\n\nFor more examples, see the `example/` directory in the YourBench repository.", "document_filename": "CONFIGURATION.md", "document_metadata": {"file_size": 11684}}
{"document_id": "8b5fa94d-e6bc-4de4-a21c-3d1b93d5c750", "document_text": "YourBench: Easy Custom Evaluation Sets for Everyone\n\nSumuk Shashidhar1,2 Clementine Fourier1 Alina Lozovskia1\nThomas Wolf1 Gokhan Tur2 Dilek Hakkani-Tür2\n1 Huggingface\n2 UIUC\nsumuks2@illinois.edu\nclementine@huggingface.co\n\nAbstract\n\nEvaluating large language models (LLMs) effectively remains a critical\nbottleneck, as traditional static benchmarks suffer from saturation and con-\ntamination, while human evaluations are costly and slow. This hinders\ntimely or domain-specific assessment, crucial for real-world applications.\nWe introduce YourBench, a novel, open-source framework that addresses\nthese limitations by enabling dynamic, automated generation of reliable,\nup-to-date, and domain-tailored benchmarks cheaply and without man-\nual annotation, directly from user-provided documents. We demonstrate\nits efficacy by replicating 7 diverse MMLU subsets using minimal source\ntext, achieving this for under $15 in total inference costs while perfectly\npreserving the relative model performance rankings (Spearman Rho = 1)\nobserved on the original benchmark. To ensure that YourBench generates\ndata grounded in provided input instead of relying on posterior parametric\nknowledge in models, we also introduce TEMPORA-0325, a novel dataset\nof over 7K diverse documents, published exclusively after March 2025.\nOur comprehensive analysis spans 26 SoTA models from 7 major fami-\nlies across varying scales (3 - 671B parameters) to validate the quality of\ngenerated evaluations through rigorous algorithmic checks (e.g., citation\ngrounding) and human assessments. We release the YourBench library, the\nTEMPORA-0325 dataset, 150k+ question answer pairs based on Tempora\nand all evaluation/inference traces to facilitate reproducible research and\nempower the community to generate bespoke benchmarks on demand,\nfostering more relevant and trustworthy LLM evaluation.\n\n1\n\nIntroduction\n\nThe rapid evolution of large language models (LLMs) continually outpaces traditional\nevaluation methodologies. Static benchmarks, foundational to earlier progress, now face\ncritical issues: they quickly saturate, are susceptible to training data contamination, become\ntemporally irrelevant as knowledge evolves, and often fail to capture model capabilities in\nspecialized domains (Kiela et al., 2021; Dominguez-Olmedo et al., 2024; Zhang et al., 2024;\nZhu et al., 2023; Ruder, 2023). While direct human assessment provides valuable insights, its\ncost and scalability limitations render it impractical for the continuous, diverse evaluation\nneeds of the field. This creates a pressing need for evaluation generation frameworks that\nare automatic, while dynamic, reliable, domain-specific, and accessible.\n\nWe therefore introduce YourBench: an open-source framework that enables automated\ngeneration of bespoke evaluation sets directly from any collection of documents. YourBench\nempowers users to systematically create fresh, relevant benchmarks tailored to specific\ntopics, achieving high reliability at low cost and without manual annotation. Central to our\nframework is the principle of Document-to-Evaluation Generation (D2EG), where LLMs are\nleveraged to produce diverse, contextually-grounded question-answer pairs with verifiable\ncitations, optimizing for coverage, diversity, and answerability (details in §2.2, Appendix C).\n\n1\n\n\fFigure 1: YourBench Automatically Generates Challenging MMLU Replicas. We eval-\nuated YourBench’s ability to replicate subsets of the MMLU benchmark across 7 diverse\ndomains (Astronomy, Anatomy, etc.). Using only a few relevant Wikipedia pages per do-\nmain as input documents, YourBench automatically generated new multiple-choice question\nsets in the MMLU style. This process took <5 minutes and <$2 of inference cost per domain,\nrequiring no human annotation. The resulting benchmarks (orange bars) demonstrate two\nkey findings: (1) They perfectly preserve the relative performance rankings of various LLMs\ncompared to the original MMLU (grey bars), confirming evaluation validity (Spearman\nρ=1.00). (2) They consistently produce harder questions (lower absolute scores), yielding a\nmore challenging, contamination-resistant evaluation derived directly from source material.\n\nWe rigorously assessed YourBench’s capability at each step, then through benchmark repli-\ncation, comparing to the widely-used MMLU dataset (Hendrycks et al., 2021a). As observed\nin Figure 1 and detailed in Section 3.3, the synthetic MMLU-style evaluation automati-\ncally generated by YourBench from minimal source text preserves the relative performance\nranking of diverse LLMs, while being harder than the initial dataset.\n\nThe framework integrates a robust pipeline (§2, Appendix A) featuring multi-format docu-\nment ingestion, semantic chunking, diverse LLM ensembles for question generation, and\nstringent automated quality controls based on citation grounding and semantic novelty.\nExtensive validation (§3.2) confirms the high quality of the generated evaluations: hu-\nman assessments show approximately 85% question validity (Appendix E.1), and models\ndemonstrate strong, efficiently achievable citation grounding (Appendix E.2, E.4). To fur-\nther support robust evaluation, particularly concerning temporal knowledge, we release\nTEMPORA-0325 (§3.1.1), a dataset comprising documents published exclusively after March\n2025, designed to mitigate contamination.\n\nOur primary contributions are:\n\n• YourBench: An open-source framework1 enabling dynamic, automated generation\n\nof reliable, domain-specific evaluation sets from documents.\n\n• TEMPORA-0325: A large-scale dataset2 of recent documents (post-March 2025) to\n\nfacilitate temporal evaluation and reduce benchmark contamination.\n\n1GitHub\n2Dataset\n\n2\n\n\f• Comprehensive Validation: Empirical demonstration of YourBench’s effectiveness\nvia benchmark replication (Figure 1), high generation quality (validity, grounding),\nand efficiency across numerous state-of-the-art LLMs.\n\nBy providing a scalable, automated, and document-grounded approach, YourBench facil-\nitates a move towards more timely, specific, and trustworthy LLM evaluation, enabling\nthe research community and practitioners alike to better understand and track the true\ncapabilities of these rapidly advancing models.\n\n2 YourBench: Multistep Framework for Dynamic Evaluation Generation\n\n2.1 Document Preprocessing\n\nTo effectively process diverse real-world documents (including various formats and multi-\nmodal content) using Large Language Models (LLMs), YourBench employs a multi-stage\npreprocessing pipeline. The primary goal is to standardize heterogeneous inputs into a\nunified, analyzable format while preserving crucial semantic and structural information.\nThis involves three key stages: (1) Document Ingestion, which normalizes formats like\nPDF, Word, and HTML into markdown and incorporates descriptions for visual content;\n(2) Semantic Chunking, which partitions documents into coherent segments to manage\ncontext length limitations and improve attention focus; and (3) Document Summarization,\nwhich generates a global overview to retain broader context often lost during chunking.\nThe detailed methodology, specific tools, models employed, and motivations for each stage\nare elaborated in Appendix B.\n\n2.2 Question and Answer Generation Process\n\n2.2.1 Overview\n\nThe process of generating evaluation questions from source documents, termed Document-\nto-Evaluation Generation (D2EG), aims to produce a question set satisfying three core criteria:\n\n1. Coverage: Address a broad range of information within the document.\n\n2. Diversity: Vary questions across difficulty, style, and reasoning type.\n\n3. Answerability & Quality: Ensure each question is unambiguously answerable\n\nfrom the source document.\n\nWhile this can be framed as a formal optimization problem (see Appendix C for the formula-\ntion using Eq. (5)), YourBench adopts a practical, greedy generation framework leveraging\nLLMs, following four main steps:\n\n1. Context Provision: Combine individual document segments ci (or multi-hop\ngroups) with the document summary s to provide both local detail and global\nperspective.\n\n2. Guided Generation: Seed LLMs with desired question types (e.g., factual, multi-\nhop, numeric) and difficulty levels (e.g., basic, advanced) to target diverse outputs.\n\n3. Ensemble Approach: Utilize a diverse collection of LLMs (varied families, sizes)\nto generate questions, harnessing different model biases to improve coverage and\ndiversity.\n\n4. Quality Filtering: Automatically filter the generated questions for clarity, con-\nsistency, and verifiable answerability using the source text, with optional human\nrefinement.\n\nThis ensemble-based, segment-parallelized approach efficiently generates a large pool of\nraw questions offering strong coverage, diversity, and textual grounding.\n\n3\n\n\f2.2.2 Approach\n\nThe transformation of preprocessed document segments into evaluation artifacts (QA pairs)\nis orchestrated via LLMs, guided by the D2EG principles (Section 2.2). Given a document d\nwith global summary S and semantic chunks C = {c1, ..., cm} (including potential multi-hop\nchunks M = {m1, ..., mp}, detailed in Appendix B.2), the core task is generating a QA pair\n(q, a) with supporting citations cit based on the context. We model this as sampling:\n\n(q, a, cit) ∼ p(·|promptgen, S, c)\n\n(1)\n\nwhere c ∈ C ∪ M is the local context chunk(s) and promptgen contains detailed instructions\n(see Appendix H).\n\nProviding both global summary S and local chunk(s) c is crucial. The local context c focuses\nthe LLM on specific details, mitigating attention diffusion issues (Liu et al., 2023; Ye et al.,\n2024), while the global summary S provides overarching context for accurate interpretation\nand relevant question formulation, especially when c alone (e.g., a table) lacks context.\n\nWithin promptgen, we instruct the LLM to dynamically adjust the quantity and variety of\nquestions based on the perceived richness of the context (S, c), rather than imposing rigid\nconstraints, to promote naturalness and satisfy D2EG criteria. We guide the model towards\nthe target JSON format using explicit instructions within the prompt, avoiding reliance on\nspecialized structured output mechanisms for broader compatibility.\n\nA key instruction in promptgen is groundedness: the model must provide citations cit (exact\nspans from c) substantiating the answer a, directly enforcing the D2EG ’Answerability &\nQuality’ constraint.\nTo enhance robustness and diversity, we employ an LLM ensemble M = {M1, ..., MN}. For\na given context (S, c), candidate QA sets Ql are generated from multiple models Ml ∈ M.\nThe final raw pool Qraw = (cid:83)N\nl=1 Ql aggregates these candidates, mitigating individual\nmodel biases and yielding a more comprehensive and diverse question set. The output\nconsists of structured candidate QA pairs (question, answer, citations), typically in JSON\nformat.\n\n2.3 Quality Filtering and Deduplication\n\nThe raw QA set Qraw from ensemble generation (Section 2.2.2) requires refinement for\nfidelity and non-redundancy. We employ a two-stage process: citation validation and\nsemantic deduplication.\n\n2.3.1 Citation Validation\n\nEnsuring QA pairs are grounded in the source context c is vital. While prompts request\ncitations cit = {c1, ..., cNc }, LLM stochasticity necessitates verification. We use an algorithmic\napproach based on fuzzy string matching (partial ratio derived from Levenshtein distance\n(Levenshtein, 1966)) to quantify the correspondence between each citation ci and the source\nchunk c. See Appendix D.1 for the detailed ‘PartialRatio‘ definition.\n\nWe assign a grounding score to each QA pair (q, a, cit) by averaging the partial ratios across\nits citations:\n\nScoreQA(q, a, cit) =\n\n1\nNc\n\nNc∑\n\ni=1\n\nPartialRatio(ci, c)\n\n(2)\n\nassuming Nc > 0 (score is 0 if Nc = 0). We filter Qraw, retaining pairs exceeding a threshold\nθcit:\n\nQcit = {(q, a, cit) ∈ Qraw | ScoreQA(q, a, cit) > θcit}\nEmpirically, θcit = 0.85 balances rigorous filtering of ungrounded pairs with preservation of\nvalid items. See Appendix D.1 for the model-level scoring metric used in evaluations.\n\n(3)\n\n4\n\n\f2.3.2 Semantic Deduplication and Reweighting\n\nEnsemble generation and chunk overlap can lead to semantic redundancy in Qcit. To manage\nthis, we perform semantic deduplication. We obtain dense embeddings e(q) for questions in\nQcit using a sentence embedding model (e.g., Sentence-BERT (Reimers & Gurevych, 2019)).\n\nWe apply DBSCAN (Ester et al., 1996), a density-based clustering algorithm, to the\nembeddings {e(q)}. DBSCAN groups semantically similar QA pairs (cosine similarity\n> τsim = 0.9) into clusters C = {C1, ..., CK} and identifies outliers N.\nFrom each cluster Ck, we select one representative QA pair (q∗\nThe deduplicated set is:\n\nk ) (e.g., the medoid).\n\nk , cit∗\n\nk , a∗\n\nQdedup = {(q∗\n\nk , a∗\n\nk , cit∗\n\nk ) | Ck ∈ C} ∪ N′\n\n(4)\n\nwhere N′ are the unique noise points.\nTo retain information about concept salience (indicated by cluster size |Ck|), we assign\nweights wk to each representative (q∗\nk ) proportional to its original cluster size (e.g.,\nwk = |Ck|), with w = 1 for noise points. These weights are used in the final evaluation\nscoring (Section 3), allowing frequently questioned concepts to contribute more significantly,\napproximating the evaluation of the full set Qcit efficiently.\n\nk , cit∗\n\nk , a∗\n\n2.4 Suggested Evaluator\n\nGiven the curated, weighted QA set Qfinal = Qdedup (Sections 2.2.2, 2.3), we generally\nevaluate free form LLMs outputs using a pairwise comparative assessment strategy (as is\ndone in model arenas). Our suggested evaluator is composed of a judge LLMs ensemble\nto enhance reliability and mitigate self-preference bias (Zheng et al., 2023), and an bias-\ncorrected scoring aggregation to mitigate positional bias (the tendency of LLMs-judges to\nprefer an answer presented in one position compared to the other). We expand on this\nin Appendix D.2. It’s also possible to use YourBench to generate questions with multiple\nchoice answers through prompt modifications, in which case it becomes possible to evaluate\nmodels through a simple exact match score, as we do in Section 3.3.\n\n3 Validating YourBench\n\n3.1 Experimental Setup\n\n3.1.1 Dataset: TEMPORA-0325\n\nA key challenge in LLM evaluation is disentangling performance derived from provided\ncontext versus memorized pretraining data. To specifically assess grounding on novel infor-\nmation and mitigate potential contamination from training on benchmark data, we introduce\nTEMPORA-0325, a dataset comprising documents published exclusively after March 1, 2025.\nEvaluating models on TEMPORA-0325 forces reliance on the provided document context,\nrevealing tendencies towards outdated parametric knowledge if inconsistencies arise.\n\nCollection Scope & Diversity. We collected 7,368 publicly available documents published\nafter March 1, 2025, spanning diverse domains (government, corporate, legal, medical,\nsports, news, blogs, miscellaneous), prioritizing factually verifiable sources. The dataset\nincludes an unbalanced full corpus reflecting real-world distributions and a balanced subset,\nTEMPORA-0325B (used in our main experiments), offering uniform coverage across eight\nsource categories for controlled analysis.\n\nBoth TEMPORA-0325 and TEMPORA-0325B are publicly available. Details on domain\nsources, data provenance, licensing, and verification are in Appendix B.4.\n\n5\n\n\f3.1.2 Model Choice\n\nTo evaluate YourBench’s question generation framework (Section 2.2), we selected a diverse\nset of 26 state-of-the-art LLMs, prioritizing variety across (1) model families (diverse pre-\ntraining data/methods), (2) parameter scales (ranging from 7B to 671B parameters), and (3)\nreasoning specialization (including models explicitly optimized for multi-step reasoning).\nOur selection includes both open-weight and closed-source API-based models (e.g., from\nDeepSeek, Qwen, Mistral, Llama, Google, OpenAI, Anthropic families). For fair comparison,\nall models used identical inputs, sampling hyperparameters, and temperature settings dur-\ning inference, with reasoning-specialized models configured to use maximum computation.\nThis allows isolating the impact of architecture and scale on generation quality.\n\n• DeepSeek (DeepSeek-AI et al., 2025b;a): DeepSeek V3 (671B), DeepSeek R1 (671B),\nDeepSeek R1-Distill-Llama (70B), and DeepSeek R1-Distill-Qwen (32B, 14B, 7B).\n• Qwen (Qwen et al., 2025): Qwen2.5 models at various scales (72B, 32B, 14B, 7B) and\n\nthe reasoning model Qwen QwQ (32B).\n\n• Mistral (Jiang et al., 2023): Mistral Large 2411 (132B) and Mistral 3.1 Small (24B).\n• Llama (Dubey et al., 2024): Llama 3.1 (405B, 8B) and Llama 3.3 (70B).\n• Google (Team et al., 2024): Gemini 2.0 Flash, Gemini 2.0 Flash Lite (?B) and Gemma\n\n3 (27B)\n\n• OpenAI (OpenAI et al., 2024): GPT-4o, GPT-4o mini, and o3 mini (?B)\n• Anthropic (Anthropic, 2024): Claude 3.7 Sonnet, Claude 3.5 Haiku (?B)\n\nTo facilitate reproducibility and further research, we open-source all inference traces for\neach evaluated model on the Tempora-0325B dataset (Section 3.1.1). This comprehensive\ncollection captures the generation process across models spanning three orders of magnitude\nin parameter count, offering insights into how different architectures approach document-\ngrounded question formulation.\n\n3.2 Generated Evaluation Quality\n\nThe practical utility of YourBench depends fundamentally on the quality, reliability, and\ncharacteristics of the evaluation sets it generates. While the introduction highlighted the\nframework’s success in replicating the MMLU benchmark (Figure 1), here we delve deeper\ninto the intrinsic properties of the generated questions, examining two crucial dimensions:\nQuestion Validity (the intrinsic correctness and answerability of a question) and Semantic\nDiversity (the breadth of topics and concepts covered). Analyzing these facets reveals not\nonly the robustness of the generated benchmarks but also offers insights into the distinct\ngenerative capabilities and \"personalities\" of different large language models.\n\n3.2.1 The Validity-Diversity Spectrum\n\nEvaluating the quality of generated questions requires understanding both their individual\nsoundness and their collective variety. To assess these aspects rigorously, we employed\ndistinct methodologies.\n\nAssessing Question Validity. A core requirement for any useful evaluation question is\nits intrinsic quality: it must be clear, sensible, and definitively answerable using only the\nprovided source material. To quantify this, we conducted a meticulous human evaluation\nprocess. We stratified sampled 2k unique questions generated across our suite of models\nfrom the TEMPORA-0325B dataset. Twenty trained annotators assessed each question\nagainst the source context based on criteria of clarity, contextual answerability, logical\nsensibility and citation answerability. Each question received three independent ratings,\nand the high inter-annotator agreement (Gwet’s AC1 = 0.71) confirmed the reliability of\nthis process. A question was deemed \"Valid\" only if it met all criteria affirmatively by\nmajority vote. Further details on the human evaluation setup and criteria are provided in\nAppendix E.1.\n\n6\n\n\fFigure 2: The Validity-Diversity Spectrum of Language Models. Comparing semantic\ndiversity scores (left) and human-annotated validity scores (right) for questions generated\nby various models reveals an intriguing trade-off. Models like o3 mini excel in validity\n(generating consistently answerable, clear questions) but exhibit low diversity, often focusing\non routine or algorithmic queries - when models like Qwen2.5 32B achieve high diversity\nbut may do so at the cost of slightly lower average validity. Some rare models, like DeepSeek\nV3, demonstrate a strong balance, scoring well on both dimensions.\n\nMeasuring Semantic Diversity. Beyond individual question quality, the value of an\nevaluation set also lies in its breadth. A diverse set probes a wider range of knowledge and\nreasoning facets present in the source documents. We measured the semantic diversity of\nthe question set generated by each model using embedding-based techniques. Questions\nwere embedded into a vector space, and we computed metrics capturing both the average\ndistance between question embeddings (dispersion) and the uniformity of their distribution\nacross semantic clusters (entropy). A combined score, normalized across models, represents\nthe overall semantic diversity. The detailed methodology is described in Appendix E.3.\n\nOur analysis, summarized in Figure 2, reveals an interplay between question validity and\nsemantic diversity across different generator models. On average, the human evaluation\nconfirmed that contemporary models integrated within YourBench can generate questions\nwith high intrinsic validity, averaging approximately 85% post-filtering across all models.\nHowever, performance varies significantly. Models like o3 mini (0.96 validity), Gemma 3\n27B (0.93), and Gemini 2.0 Flash (0.91) demonstrate exceptional ability to produce ques-\ntions that are clear, contextually grounded, and sensible according to human judgment.\nSimultaneously, examining semantic diversity shows a different ranking. Models such as\nQwen2.5 32B (0.93 diversity), DeepSeek V3 671B (0.90), and Qwen2.5 72B (0.89) excel at\ngenerating questions that span a wide range of topics and concepts extracted from the\ndocuments. Further analysis exploring the relationship between generation cost, model size,\nand validity is available in Appendix E.4.\n\n3.2.2 Citation Grounding\n\nFaithful attribution to source material via citations is crucial for verifying the grounding\nof generated answers. YourBench incorporates automated citation validation using fuzzy\nstring matching (detailed in §2.3 and Appendix E.2). To assess different models’ proficiency\nin this, we computed an aggregate citation score reflecting the average grounding quality\nacross their generated QA pairs.\n\nFigure 3 presents the results. Panel (a) shows that leading models like Claude 3.7 Sonnet and\nseveral competitive open-weight models (e.g., from Qwen, Gemma families) demonstrate\n\n7\n\n\f(a) Model citation scores.\n\n(b) Inference cost vs. Citation score.\n\nFigure 3: Evaluation of citation grounding performance. (a) Compares aggregate citation\nscores across various models. (b) Illustrates the Pareto frontier for inference cost (log scale)\nversus citation score, highlighting efficiency trade-offs. Full model list in Appendix D.3.\n\nstrong citation generation capabilities. Panel (b), plotting inference cost against citation\nscore, reveals significant efficiency trade-offs. Models like Qwen2.5 32B achieve high citation\nvalidity at a fraction of the cost of the top performers, indicating that reliable grounding is\nattainable efficiently within the YourBench framework. This suggests citation quality can\nserve as a valuable and cost-effective evaluation signal. Detailed scores and cost analysis\nmethodology are in Appendix E.2.\n\n3.3 End to end validation: MMLU Replication\n\nAs introduced in §1 and illustrated in Figure 1, we aimed to validate the YourBench frame-\nwork by automatically replicating subsets of the MMLU benchmark (Hendrycks et al.,\n2021a). To do so, we evaluated a suite of 8 LLMs (see Table 1 in Appendix) on 7 original\nMMLU subject subsets and their corresponding YourBench-generated counterparts, created\nfrom sampled Wikipedia documents of relevant topics for each subset. We provide some\nside by side examples in Fig 4.\n\nWe then analyzed the correlation between the performance scores (accuracy) obtained on\nthe original versus the synthetic benchmarks. The correlation analysis between original\nMMLU subject subsets and their YourBench counterparts revealed two key findings: (1)\nAt the individual subject-model level (56 pairs), correlation was positive but moderate\n(Pearson r=0.3833, p=0.0035; Spearman ρ=0.2982, p=0.0256), suggesting some variance\nin specific subject measurements. (2) When examining mean model performance (7 data\npoints), the correlation became remarkably strong (Pearson r=0.9646, p<0.0001; Spear-\nman ρ=1.0000, p<0.0001), demonstrating that while YourBench questions appear more\nchallenging, they preserve the relative ranking of models perfectly. This key finding\ndemonstrates that YourBench reliably captures the relative capabilities of different LLMs,\nmirroring the discriminative power of the original MMLU, while generating fresh, poten-\ntially contamination-resistant questions. Comprehensive correlation statistics and detailed\nper-subject performance tables generated from our evaluation suite are provided in Ap-\npendix F.\n\n8\n\n\f4 Related Work\n\nEvaluating large language models (LLMs) presents significant challenges that motivate\nYourBench. Traditional static benchmarks (Deng, 2012) face issues of saturation, as models\nquickly reach performance ceilings (Ruder, 2023; Wei, 2023), and contamination, where test\ndata leaks into training sets, inflating scores (Kiela et al., 2021; Zhang et al., 2024). Their\nfixed nature also leads to temporal irrelevance due to evolving world knowledge (Zhu\net al., 2023; Deng et al., 2024) and poor suitability for assessing domain-specific capabilities.\nThese limitations underscore the need for dynamic, robust evaluation methods reflecting\nreal-world data.\n\nResponses include dynamic benchmark generation, like Dynabench (Kiela et al., 2021),\nwhich faces scaling issues, and synthetic generation using LLMs (Wei, 2023; Krishna et al.,\n2024; Ruder, 2023), which struggles with quality control and grounding (Zhou et al., 2025).\nDomain-specific benchmarks (Hung et al., 2023a; Nori et al., 2023; Holzenkamp et al.,\n2023) improve relevance but are often costly, static, and lack continuous updates (Zhang\net al., 2024). Persistent gaps remain in creating scalable, reliable, diverse, and temporally-\naware evaluations grounded in specific document sources. YourBench addresses these by\nproviding an adaptive, document-driven framework for generating fresh, domain-specific,\nand contamination-resistant evaluation sets on demand. We only provided the high level\nview of the related works here, but a more extensive and comprehensive discussion of the\nliterature is detailed in Appendix G.\n\n5 Conclusion and Initial Applications\n\nWe introduced YourBench, an open-source framework for the automated generation of\ndocument-grounded evaluation sets, addressing key limitations of static benchmarks and\nmanual evaluation in assessing LLMs. Our validation demonstrated YourBench’s ability\nto efficiently produce reliable, challenging, and domain-specific benchmarks—capable of\nreplicating established evaluation results like MMLU rankings—without manual annotation\nrequired in the process (§3, Appendix F).\n\nThe framework’s potential extends beyond benchmark replication and is already being\nexplored in several research initiatives:\n\n• Domain-Specific Knowledge Assessment (Agriculture): YourBench is being uti-\nlized to systematically evaluate LLMs on specialized, proprietary knowledge. This\ninvolves generating questions assessing factual recall, applied reasoning, and\n\nFigure 4: Comparison of generated MMLU style questions in various domains.\n\n9\n\n\fretrieval-augmented generation capabilities based on diverse agricultural docu-\nments, effectively measuring a model’s domain intelligence\n\n• Personalized Education: In educational research, the framework is being adapted\nto assist teachers and generate tailored assessment questions based on individual\nstudent learning profiles derived from textual inputs, exploring pathways towards\nautomated, personalized learning tools.\n\n• Advanced RAG Training Data: YourBench’s capacity for multi-hop question gen-\neration is being employed to construct challenging training corpora for retrieval-\naugmented generation systems. By synthesizing complex questions requiring in-\nformation integration across multiple document chunks and incorporating human\nfeedback loops, this effort aims to push the boundaries of RAG model capabilities.\n\nBy providing a robust, scalable and fast automated approach, YourBench facilitates more nu-\nanced, timely, and targeted assessments of LLM capabilities, at a low cost (which makes the\nprocess accessible to most). We believe such tools will help drive deeper understanding and\nfostering continued, responsible progress in language model development and application\nacross diverse fields.\n\nReproducibility\n\nWe are committed to ensuring the reproducibility of our research and facilitating further\ninvestigation by the community. To this end, we make several key resources publicly\navailable. The complete source code for the YourBench framework is released under an\nopen-source license and can be accessed at https://github.com/huggingface/yourbench.\nThis repository includes the implementation of the document processing pipeline (Section\n2.1), the question generation framework (Section 2.2), and associated evaluation scripts.\n\nFurthermore, the TEMPORA-0325 dataset introduced in Section 3.1.1, comprising documents\npublished after March 1, 2025, is available on the Hugging Face Hub at this datasets link.\nAlongside the dataset, we provide the code used for document collection, preprocessing,\nsemantic chunking (Section B.2), and subsequent analysis within the main framework\nrepository.\n\nTo enable detailed verification of our experimental findings, we release the complete in-\nference traces for critical experiments, including the MMLU replication study (Section 3.3)\nand the citation validity analysis (Figure 3). These traces cover the diverse set of 26 large\nlanguage models detailed in Section 3, spanning both open-weight models (e.g., Llama,\nQwen, DeepSeek families) and closed-source API-based models (e.g., GPT, Claude, Gemini\nfamilies). Our inclusion of both model types is a deliberate choice to enhance long-term\nreproducibility; by providing results for open models, we ensure that future researchers\ncan replicate or extend our core findings even if commercial APIs become deprecated or\nchange significantly over time. All code and experimental artifacts are designed to support\ntransparency and allow the community to build upon our work effectively.\n\nEthical Considerations\n\nThe development of powerful AI systems necessitates equally robust and trustworthy\nmethods for their evaluation. Frameworks like YourBench, which automate the generation\nof evaluation benchmarks, represent a step towards more dynamic and potentially less\ncontaminated assessment. However, like any technology, its introduction warrants careful\nconsideration of the ethical dimensions and potential societal impacts.\n\nOne important area relates to the human element in data creation. Traditionally, benchmark\ncreation involves significant human labor, often in the form of detailed annotation or ques-\ntion writing. This labor, while essential, can sometimes be repetitive and subject to economic\npressures, including concerns about fair compensation, particularly in globally distributed\nworkforces. YourBench introduces a potential shift in this dynamic. By automating the\ngeneration of question-answer pairs, the burden on humans might transition from primarily\n\n10\n\n\fgenerative tasks to ones involving oversight, validation, and curation. Instead of authoring\nquestions from scratch, the focus could shift towards assessing the quality, relevance, and\nsafety of machine-generated content, or guiding the generation process towards specific\nevaluation goals. It’s uncertain as of now whether such a shift would rather elevate the\nnature of the work, (demanding more critical judgment rather than repetitive production),\nor simply remove large-scale, low-wage annotators from the equation by replacing them\nwith skilled annotators. It requires careful consideration and proactive effort to ensure that\nindividuals involved are equipped with the necessary skills for these evolving roles and\nthat the economic benefits of automation are shared equitably. The potential for deskilling\nor displacement in certain areas must also be acknowledged and addressed thoughtfully by\nthe community and organizations deploying such systems. We must remain mindful of the\nhuman collaborators whose insights remain crucial, even as the tools evolve.\n\nFurthermore, the integrity of the evaluation process itself relies heavily on the quality and\ncharacteristics of the LLMs used within the YourBench framework. The models employed\nfor generating questions, summaries, and even judging responses inevitably embed their\nown biases, limitations, and potential failure modes, learned from their own training\ndata. If not carefully managed, YourBench could inadvertently propagate or even amplify\nthese biases within the generated benchmarks. This underscores the critical importance of\ntransparency regarding the models used in the generation process and the need for robust,\nongoing validation of the generated datasets – not just for correctness, but also for fairness,\nrepresentation, and potential hidden biases. Automated checks, like the citation grounding\nimplemented, are valuable, but human oversight remains essential for identifying more\nsubtle issues.\n\nThe increased accessibility offered by YourBench, allowing for rapid generation of domain-\nspecific benchmarks, is a significant advantage. It empowers researchers and practitioners\nto create evaluations tailored to their specific needs, moving beyond generic, potentially\nsaturated benchmarks. However, this ease of creation also carries a potential for misuse.\nBenchmarks could conceivably be generated to specifically highlight the strengths or weak-\nnesses of particular models, potentially leading to misleading comparisons if not used\nresponsibly and transparently.\n\nFinally, the computational resources required to run ensembles of large models for genera-\ntion and evaluation contribute to the environmental footprint of AI development. While\nYourBench might offer efficiencies compared to certain manual processes or continuous\nlarge-scale human evaluations, the aggregate energy consumption remains a factor worthy\nof consideration as such automated systems become more widespread.\n\nIn conclusion, while YourBench offers a promising direction for advancing LLM evalua-\ntion, its development and deployment must proceed with a deep sense of responsibility.\nContinuous monitoring of its impacts, particularly on human labor dynamics and the in-\ntegrity of evaluation results, is essential. The goal should not merely be automation, but the\ncreation of evaluation methodologies that are not only more efficient and relevant but also\nfundamentally fair, trustworthy, and aligned with the broader goal of developing beneficial\nAI.\n\nAcknowledgements\n\nThis research project has benefited from the Microsoft Accelerate Foundation Models Re-\nsearch (AFMR) grant program through which leading foundation models hosted by Mi-\ncrosoft Azure, along with access to Azure credits, were provided to conduct the research.\nAdditionally, this research utilized Anthropic credits granted through Anthropic’s External\nResearcher Access Program. This research used the Delta advanced computing and data\nresource, supported by the National Science Foundation (award OAC 2005572) and the\nState of Illinois; Delta is a joint effort of the University of Illinois Urbana-Champaign and its\nNational Center for Supercomputing Applications. We also gratefully acknowledge Hug-\nging Face for supporting inference costs, as well as SambaNova and Novita for providing\ninference services.\n\n11\n\n\fReferences\n\nAnthropic. The claude 3 model family: Opus, sonnet, haiku. Technical report, Anthropic,\n\nMarch 2024. URL https://www.anthropic.com/news/claude-3-family.\n\nRalph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. the\n\nmethod of paired comparisons. Biometrika, 39(3/4):324–345, 1952.\n\nYapei Chang, Kyle Lo, Tanya Goyal, and Mohit Iyyer. Booookscore: A systematic exploration\nof book-length summarization in the era of llms, 2024. URL https://arxiv.org/abs/\n2310.00785.\n\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz\nKaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher\nHesse, and John Schulman. Training verifiers to solve math word problems, 2021. URL\nhttps://arxiv.org/abs/2110.14168.\n\nDeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin\nXu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu,\nZ. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan\nWang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu\nZhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong\nDai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu,\nHaocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong\nGuo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L.\nCai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin\nHuang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang,\nLei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun\nWang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu\nChen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L.\nJin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu\nWang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu,\nShengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao,\nWen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An,\nXiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie,\nXingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin,\nXiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou,\nXianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao\nLi, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong,\nYing He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo,\nYuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo,\nYuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui\nLi, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren,\nZehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao,\nZhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie,\nZiyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang.\nDeepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025a.\nURL https://arxiv.org/abs/2501.12948.\n\nDeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu,\nChenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo,\nDejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo,\nGuangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng\nWang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L.\nCai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang\nChen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao,\nKang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang\nZhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua\nZhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang,\nQiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge,\nRuisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li,\n\n12\n\n\fShanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng\nYe, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan,\nT. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao, Wei\nAn, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue\nJin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen,\nXiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng,\nXin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu\nYang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu,\nYang Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun,\nYaohui Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying\nHe, Ying Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang\nGuo, Yu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He,\nYukun Zha, Yunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan\nLiu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen\nHuang, Zhen Zhang, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng\nMa, Zhigang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li,\nZihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Ziyi Gao, and Zizheng\nPan. Deepseek-v3 technical report, 2025b. URL https://arxiv.org/abs/2412.19437.\n\nLi Deng. The mnist database of handwritten digit images for machine learning research.\n\nIEEE Signal Processing Magazine, 29(6):141–142, 2012.\n\nP. Deng, J. Wang, and T. Zhao. Newterm: Benchmarking real-time new terms for large\n\nlanguage models with annual updates. https://arxiv.org/abs/2410.20814, 2024.\n\nRicardo Dominguez-Olmedo, Florian E. Dorner, and Moritz Hardt. Training on the test task\nconfounds evaluation and emergence, 2024. URL https://arxiv.org/abs/2407.07890.\n\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle,\nAiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal,\nAnthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev,\nArthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava\nSpataru, Baptiste Roziere, Bethany Biron, et al. The llama 3 herd of models, 2024. URL\nhttps://arxiv.org/abs/2407.21783.\n\nArpad E. Elo. The Rating of Chessplayers, Past and Present. Arco Publishing, New York, 1978.\n\nMartin Ester, Hans-Peter Kriegel, Jorg Sander, Xiaowei Xu, et al. A density-based algorithm\nfor discovering clusters in large spatial databases with noise. In kdd, volume 96, pp.\n226–231, 1996.\n\nClémentine Fourrier, Nathan Habib, Hynek Kydlíˇcek, Thomas Wolf, and Lewis Tunstall.\nLighteval: A lightweight framework for llm evaluation, 2023. URL https://github.com/\nhuggingface/lighteval.\n\nVipul Gupta, David Pantoja, Candace Ross, Adina Williams, and Megan Ung. Changing\nanswer order can decrease mmlu accuracy, 2024. URL https://arxiv.org/abs/2406.\n19470.\n\nKilem L. Gwet. Computing inter-rater reliability and its variance in the presence of high\nagreement. British Journal of Mathematical and Statistical Psychology, 61(1):29–48, 2008. doi:\n10.1348/000711006X126600.\n\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt. Measuring massive multitask language understanding, 2021a. URL\nhttps://arxiv.org/abs/2009.03300.\n\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang,\nDawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the\nmath dataset, 2021b. URL https://arxiv.org/abs/2103.03874.\n\n13\n\n\fAnna Holzenkamp, R. Ghosh, and D. et al. Zhang. Legalbench: A collaboratively built\nbenchmark for measuring legal reasoning in large language models. https://arxiv.org/\nabs/2308.11462, 2023.\n\nJ. Hung, N. Parekh, and T. Yun. High risk domains in llm benchmarking.\n\n//aclanthology.org/2023.genbench-1.8.pdf, 2023a.\n\nhttps:\n\nK. Hung, T. Roy, and D. Marino. Limitations of llms for high-risk domains despite domain-\nspecific instruction tuning. https://aclanthology.org/2023.genbench-1.8.pdf, 2023b.\n\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\nSaulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut\nLavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023. URL\nhttps://arxiv.org/abs/2310.06825.\n\nDouwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu,\nBertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, Zhiyi Ma, Tristan\nThrush, Sebastian Riedel, Zeerak Waseem, Pontus Stenetorp, Robin Jia, Mohit Bansal,\nChristopher Potts, and Adina Williams. Dynabench: Rethinking benchmarking in NLP.\nIn Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy,\nSteven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (eds.), Proceedings\nof the 2021 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pp. 4110–4124, Online, June 2021. Association\nfor Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.324. URL https:\n//aclanthology.org/2021.naacl-main.324.\n\nSatyapriya Krishna, Kalpesh Krishna, Anhad Mohananey, Steven Schwarcz, Adam Stambler,\nShyam Upadhyay, and Manaal Faruqui. Fact, fetch, and reason: A unified evaluation of\nretrieval-augmented generation, 2024. URL https://arxiv.org/abs/2409.12941.\n\nJ. Richard Landis and Gary G. Koch. The measurement of observer agreement for categorical\n\ndata. Biometrics, 33(1):159–174, 1977. doi: 10.2307/2529310.\n\nVladimir I. Levenshtein. Binary codes capable of correcting deletions, insertions, and\nreversals. Soviet Physics Doklady, 10(8):707–710, 1966. Translated from Doklady Akademii\nNauk SSSR, Vol. 163 No. 4 pp. 845–848, 1965.\n\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni,\nand Percy Liang. Lost in the middle: How language models use long contexts, 2023. URL\nhttps://arxiv.org/abs/2307.03172.\n\nHarsha Nori, Nick King, and Scott M. et al. McKinney. Capabilities of gpt-4 on medical\n\nexams and clinical vignettes. https://arxiv.org/abs/2303.13375, 2023.\n\nOpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh,\nAidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander M ˛adry,\nAlex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov,\nAlex Nichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi\nChristakis, Alexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou\nCrookes, Amin Tootoochian, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, An-\ndrej Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu,\nAndrew Kondrich, Andrew Tulloch, Andrey Mishchenko, Angela Baek, Angela Jiang,\nAntoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi\nNayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Leimberger, Ben Rossen, Ben\nSokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob McGrew, Bobby\nSpero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan Quinn,\nBrian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll\nWainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun\nShern, Channing Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong\nZhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim,\n\n14\n\n\fChristine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Win-\nter, Coley Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, Dane Sherburn,\nDaniel Kappler, Daniel Levin, Daniel Levy, David Carr, David Farhi, David Mely, David\nRobinson, David Sasaki, Denny Jin, Dev Valladares, Dimitris Tsipras, Doug Li, Duc Phong\nNguyen, Duncan Findlay, Edede Oiwoh, Edmund Wong, Ehsan Asdar, Elizabeth Proehl,\nElizabeth Yang, Eric Antonow, Eric Kramer, Eric Peterson, Eric Sigler, Eric Wallace, Eugene\nBrevdo, Evan Mays, Farzad Khorasani, Felipe Petroski Such, Filippo Raso, Francis Zhang,\nFred von Lohmann, Freddie Sulit, Gabriel Goh, Gene Oden, Geoff Salmon, Giulio Starace,\nGreg Brockman, Hadi Salman, Haiming Bao, Haitang Hu, Hannah Wong, Haoyu Wang,\nHeather Schmidt, Heather Whitney, Heewoo Jun, Hendrik Kirchner, Henrique Ponde\nde Oliveira Pinto, Hongyu Ren, Huiwen Chang, Hyung Won Chung, Ian Kivlichan, Ian\nO’Connell, Ian O’Connell, Ian Osband, Ian Silber, Ian Sohl, Ibrahim Okuyucu, Ikai Lan,\nIlya Kostrikov, Ilya Sutskever, Ingmar Kanitscheider, Ishaan Gulrajani, Jacob Coxon, Jacob\nMenick, Jakub Pachocki, James Aung, James Betker, James Crooks, James Lennon, Jamie\nKiros, Jan Leike, Jane Park, Jason Kwon, Jason Phang, Jason Teplitz, Jason Wei, Jason\nWolfe, Jay Chen, Jeff Harris, Jenia Varavva, Jessica Gan Lee, Jessica Shieh, Ji Lin, Jiahui Yu,\nJiayi Weng, Jie Tang, Jieqi Yu, Joanne Jang, Joaquin Quinonero Candela, Joe Beutler, Joe\nLanders, Joel Parish, Johannes Heidecke, John Schulman, Jonathan Lachman, Jonathan\nMcKay, Jonathan Uesato, Jonathan Ward, Jong Wook Kim, Joost Huizinga, Jordan Sitkin,\nJos Kraaijeveld, Josh Gross, Josh Kaplan, Josh Snyder, Joshua Achiam, Joy Jiao, Joyce\nLee, Juntang Zhuang, Justyn Harriman, Kai Fricke, Kai Hayashi, Karan Singhal, Katy\nShi, Kavin Karthik, Kayla Wood, Kendra Rimbach, Kenny Hsu, Kenny Nguyen, Keren\nGu-Lemberg, Kevin Button, Kevin Liu, Kiel Howe, Krithika Muthukumar, Kyle Luther,\nLama Ahmad, Larry Kai, Lauren Itow, Lauren Workman, Leher Pathak, Leo Chen, Li Jing,\nLia Guy, Liam Fedus, Liang Zhou, Lien Mamitsuka, Lilian Weng, Lindsay McCallum,\nLindsey Held, Long Ouyang, Louis Feuvrier, Lu Zhang, Lukas Kondraciuk, Lukasz\nKaiser, Luke Hewitt, Luke Metz, Lyric Doshi, Mada Aflak, Maddie Simens, Madelaine\nBoyd, Madeleine Thompson, Marat Dukhan, Mark Chen, Mark Gray, Mark Hudnall,\nMarvin Zhang, Marwan Aljubeh, Mateusz Litwin, Matthew Zeng, Max Johnson, Maya\nShetty, Mayank Gupta, Meghan Shah, Mehmet Yatbaz, Meng Jia Yang, Mengchao Zhong,\nMia Glaese, Mianna Chen, Michael Janner, Michael Lampe, Michael Petrov, Michael Wu,\nMichele Wang, Michelle Fradin, Michelle Pokrass, Miguel Castro, Miguel Oom Temudo\nde Castro, Mikhail Pavlov, Miles Brundage, Miles Wang, Minal Khan, Mira Murati,\nMo Bavarian, Molly Lin, Murat Yesildal, Nacho Soto, Natalia Gimelshein, Natalie Cone,\nNatalie Staudacher, Natalie Summers, Natan LaFontaine, Neil Chowdhury, Nick Ryder,\nNick Stathas, Nick Turley, Nik Tezak, Niko Felix, Nithanth Kudige, Nitish Keskar, Noah\nDeutsch, Noel Bundick, Nora Puckett, Ofir Nachum, Ola Okelola, Oleg Boiko, Oleg\nMurk, Oliver Jaffe, Olivia Watkins, Olivier Godement, Owen Campbell-Moore, Patrick\nChao, Paul McMillan, Pavel Belov, Peng Su, Peter Bak, Peter Bakkum, Peter Deng, Peter\nDolan, Peter Hoeschele, Peter Welinder, Phil Tillet, Philip Pronin, Philippe Tillet, Prafulla\nDhariwal, Qiming Yuan, Rachel Dias, Rachel Lim, Rahul Arora, Rajan Troll, Randall Lin,\nRapha Gontijo Lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza\nZamani, Ricky Wang, Rob Donnelly, Rob Honsby, Rocky Smith, Rohan Sahai, Rohit\nRamchandani, Romain Huet, Rory Carmichael, Rowan Zellers, Roy Chen, Ruby Chen,\nRuslan Nigmatullin, Ryan Cheu, Saachi Jain, Sam Altman, Sam Schoenholz, Sam Toizer,\nSamuel Miserendino, Sandhini Agarwal, Sara Culver, Scott Ethersmith, Scott Gray, Sean\nGrove, Sean Metzger, Shamez Hermani, Shantanu Jain, Shengjia Zhao, Sherwin Wu,\nShino Jomoto, Shirong Wu, Shuaiqi, Xia, Sonia Phene, Spencer Papay, Srinivas Narayanan,\nSteve Coffey, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tao Xu, Tarun\nGogineni, Taya Christianson, Ted Sanders, Tejal Patwardhan, Thomas Cunninghman,\nThomas Degry, Thomas Dimson, Thomas Raoux, Thomas Shadwell, Tianhao Zheng, Todd\nUnderwood, Todor Markov, Toki Sherbakov, Tom Rubin, Tom Stasi, Tomer Kaftan, Tristan\nHeywood, Troy Peterson, Tyce Walters, Tyna Eloundou, Valerie Qi, Veit Moeller, Vinnie\nMonaco, Vishal Kuo, Vlad Fomenko, Wayne Chang, Weiyi Zheng, Wenda Zhou, Wesam\nManassra, Will Sheu, Wojciech Zaremba, Yash Patil, Yilei Qian, Yongjik Kim, Youlong\nCheng, Yu Zhang, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, and Yury Malkov.\nGpt-4o system card, 2024. URL https://arxiv.org/abs/2410.21276.\n\n15\n\n\fQwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,\nChengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu,\nJianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming\nLu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men,\nRunji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang\nFan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan\nQiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115.\n\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+\nquestions for machine comprehension of text, 2016. URL https://arxiv.org/abs/1606.\n05250.\n\nNils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese\n\nbert-networks, 2019. URL https://arxiv.org/abs/1908.10084.\n\nSebastian Ruder. The evolving landscape of llm evaluation. https://newsletter.ruder.\n\nio/p/the-evolving-landscape-of-llm-evaluation, 2023.\n\nSumuk Shashidhar, Abhinav Chinta, Vaibhav Sahai, Zhenhailong Wang, and Heng Ji.\nDemocratizing llms: An exploration of cost-performance trade-offs in self-refined open-\nsource models. In Findings of the Association for Computational Linguistics: EMNLP 2023,\npp. 9070–9084. Association for Computational Linguistics, 2023. doi: 10.18653/v1/2023.\nfindings-emnlp.608. URL http://dx.doi.org/10.18653/v1/2023.findings-emnlp.608.\n\nGemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju,\nShreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya\nTafti, Léonard Hussenot, Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts,\nAditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amélie Héliou, Andrea\nTacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le\nLan, Christopher A. Choquette-Choo, Clément Crepy, Daniel Cer, Daphne Ippolito,\nDavid Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-\nChristian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan\nGrishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau,\nJeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones,\nKatherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon,\nMachel Reid, Maciej Mikuła, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum\nThain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko\nYotov, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy,\nRuibo Liu, Ryan Mullins, Samuel L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto\nDouglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan,\nVlad Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao Gong, Tris\nWarkentin, Ludovic Peran, Minh Giang, Clément Farabet, Oriol Vinyals, Jeff Dean, Koray\nKavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando\nPereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen\nKenealy. Gemma: Open models based on gemini research and technology, 2024. URL\nhttps://arxiv.org/abs/2403.08295.\n\nQwen Team. Qwen2.5-vl, January 2025. URL https://qwenlm.github.io/blog/qwen2.\n\n5-vl/.\n\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\nGlue: A multi-task benchmark and analysis platform for natural language understanding,\n2019. URL https://arxiv.org/abs/1804.07461.\n\nFeng Wang, Zesheng Shi, Bo Wang, Nan Wang, and Han Xiao. Readerlm-v2: Small language\nmodel for html to markdown and json, 2025. URL https://arxiv.org/abs/2503.01151.\nJason Wei. Successful language model evals. https://www.jasonwei.net/blog/evals, 2023.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi,\nQuoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large lan-\nguage models, 2023. URL https://arxiv.org/abs/2201.11903.\n\n16\n\n\fTianzhu Ye, Li Dong, Yuqing Xia, Yutao Sun, Yi Zhu, Gao Huang, and Furu Wei. Differential\n\ntransformer, 2024. URL https://arxiv.org/abs/2410.05258.\n\nHugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, Will Song, Tiffany Zhao,\nPranav Raja, Dylan Slack, Qin Lyu, Sean Hendryx, Russell Kaplan, Michele Lunati, and\nSummer Yue. A careful examination of large language model performance on grade\nschool arithmetic, 2024. URL https://arxiv.org/abs/2405.00332.\n\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao\nZhuang, Zi Lin, Zhuohan Li, and Ion Stoica. Judging LLM-as-a-judge with MT-Bench\nand chatbot arena. In Proc. of NeurIPS (Poster), 2023.\n\nWei Zhou, Alan Chen, and Zheng et al. Tan. Recent advances in large language model\nbenchmarks against data contamination: From static to dynamic evaluation. https:\n//arxiv.org/html/2502.17521v1, 2025.\n\nX. Zhu, W. Li, and G. Peng. Is your llm outdated? evaluating llms at temporal generalization.\n\nhttps://arxiv.org/html/2405.08460v2, 2023.\n\n17\n\n\fA YourBench Pipeline Overview\n\nFigure 5 provides a high-level schematic of the end-to-end YourBench framework. The\nprocess begins with ingesting diverse source documents, which are then preprocessed\nthrough steps like semantic chunking and summarization (§2.1, Appendix B). An ensemble\nof LLMs generates raw question-answer pairs grounded in the document chunks, guided\nby principles aiming for coverage, diversity, and answerability (§2.2, Appendix C). These\nraw outputs undergo rigorous quality filtering, including citation validation and semantic\ndeduplication, to produce a high-fidelity evaluation set (§2.3). Finally, this curated set is\nused within an automated evaluation framework, typically employing an ensemble of LLM\njudges to rank the performance of target models (§3). This modular pipeline allows for\nflexibility and robust, automated benchmark creation from arbitrary document inputs.\n\nFigure 5: Overview of the YourBench Framework: A dynamic pipeline starting from diverse\ndocuments, through preprocessing (ingestion, chunking, summarization - §2.1), LLM-driven\nquestion generation (following D2EG principles - §2.2), quality filtering (citation validation,\ndeduplication - §2.3), to automated evaluation using an LLM judge ensemble (§3).\n\nB Detailed Document Preprocessing\n\nThis appendix details the multi-stage preprocessing pipeline used in YourBench, designed\nto convert diverse, real-world documents into a standardized format suitable for LLM-based\nquestion generation, as summarized in Section 2.1. The pipeline addresses challenges posed\nby heterogeneous formats and multimodal content.\n\nB.1 Document Ingestion\n\nWe implement a unified ingestion pipeline using ReaderLM-v2 (Wang et al., 2025) (stripping\nknown HTML content) and Markitdown3 (converting various document types like PDF and\nWord into markdown). This approach retains key structural elements (headings, lists, tables,\nmath) while simplifying complex layouts into a standard text-based markdown syntax,\nensuring consistency across sources.\n\nWhen visual content (e.g., images) is present, we generate high-level descriptions using\nQwen2.5-72B-VL (Team, 2025) for captioning. These descriptions are incorporated into the\nmarkdown representation, allowing question generation modules to reference both textual\nand visual information uniformly. An example of a multimodal document input is shown\nin Appendix B.4 (Figure 6).\n\nB.2 Semantic Chunking\n\nProcessing full documents directly with LLMs presents challenges, including attention\ndispersion potentially overlooking content (Ye et al., 2024), and performance degradation\nwith longer contexts (Liu et al., 2023).\n\n3https://github.com/microsoft/markitdown\n\n18\n\n\fWe address these through semantic chunking, which partitions documents into coherent\nsegments. This process involves decomposing the document into sentences, computing\nembeddings, and then splitting the text into chunks based on semantic similarity and token\nlength constraints, preserving coherence within and across segments. Multi-hop chunking\nis also implemented by combining multiple non-contiguous chunks to facilitate questions\nrequiring information synthesis across different document parts.\nGiven a document d, we first decompose it into sentences S = {s1, ..., sn} and compute\ntheir embeddings E = {e1, ..., en} using a sentence transformer model (Reimers & Gurevych,\n2019), where ei ∈ Rk. The chunking process is governed by three parameters: lmin: minimum\nchunk length in tokens, lmax: maximum chunk length in tokens, and τ: similarity threshold\nfor chunk boundaries. For consecutive sentences si and si+1, we compute their semantic\nsimilarity using cosine similarity:\n\nsim(si, si+1) =\n\nei · ei+1\n∥ei∥∥ei+1∥\n\nA chunk boundary is established at position i when the current chunk’s token length exceeds\nlmin AND either sim(si, si+1) < τ OR appending si+1 would cause the accumulated chunk\nlength to exceed lmax. This process yields a set of text chunks C = {c1, ..., cm} where each\nchunk cj is a contiguous sequence of sentences from S.\n\nMultihop Chunking: To enable the generation of questions requiring synthesis across\nmultiple document segments, we implement multihop chunking. Given parameters hmin\nand hmax (minimum and maximum number of hops), we generate composite chunks. For\neach multihop chunk, we sample k ∼ U (hmin, hmax) original chunks uniformly without\nreplacement from C and concatenate their text content. This produces a set of multihop\nchunks M = {m1, ..., mp} where each mi consists of k potentially non-contiguous original\nchunks. These multihop chunks are used alongside the original chunks C during question\ngeneration (Section 2.2.2). appendix context\n\nB.3 Document Summarization\n\nWhile chunking manages context length, it can lead to a loss of global document perspective\nduring question generation. To mitigate this, we generate a document-wide summary using\nan LLM (DeepSeek-V3 (DeepSeek-AI et al., 2025b) with zero temperature). For extremely\nlong documents exceeding context limits, techniques like those in (Chang et al., 2024)\ncan be employed. Our summarization uses chain-of-thought prompting (Wei et al., 2023)\nwith structured XML tags4 for quality and consistency. This concise summary is provided\nalongside individual chunks (Section 2.2.2) to give the question generation LLM both local\ndetail and global context. The full summarization prompt is available in Appendix H.\n\nB.4 Sample Document\n\nFigure 6 shows an example document typical of those included in the dataset, featuring a\nmix of text and visual elements handled by our preprocessing pipeline (Appendix B).\n\n4https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/use-xml-tags\n\n19\n\n\fFigure 6: Example of a contemporary multimodal document included in Tempora-0325\n\nC Theoretical framework underlying the data generation work\n\nThis appendix outlines the theoretical foundation for automated benchmark generation\nfrom source documents within the YourBench framework, termed Document-to-Evaluation\nGeneration (D2EG), as introduced conceptually in Section 2.2. The goal is to produce a\nrepresentative question set Q derived from a source document (partitioned into segments\n{c1, . . . , cm} and optionally summarized by s) that satisfies key desiderata.\nLet Q be the universe of all possible questions derivable from the document segments. We\nseek a subset Q ⊆ Q that optimizes the trade-off between:\n\n1. Minimality: Penalizing the total number of questions |Q| for efficiency.\n2. Coverage: Maximizing the extent to which Q addresses the source material.\n3. Diversity: Ensuring variety in question type, difficulty, and targeted reasoning\n\nskills.\n\n4. Answerability & Quality: A hard constraint ensuring every q ∈ Q is valid and\n\nverifiably answerable from the source.\n\nThis can be formalized conceptually as a constrained optimization problem:\n\nL(Q) = α\n\n(cid:12)\n(cid:12)Q\n\n(cid:12)\n(cid:12) + β Luncov(Q) + γ Lunif(Q),\n\nmin\nQ⊆Q\n\n(5)\n\nsubject to the constraint that every question in Q is verifiably answerable from the source\ndocument. Here, Luncov(Q) penalizes the amount of source material left uncovered by\nQ, while Lunif(Q) penalizes lack of diversity (e.g., high semantic overlap) within Q. The\nnon-negative coefficients α, β, γ balance these competing objectives.\n\nFinding an exact solution to (5) is generally intractable due to the combinatorial nature of\nselecting Q from Q. Therefore, as described in Section 2.2, YourBench employs a practical,\ngreedy generation framework using LLMs, guided by prompts and context, to approximate\na desirable question set Q that adheres to the D2EG principles.\n\n20\n\n\fD Framework Theoretical Complements\n\nD.1 Citation validity\n\nTo validate the grounding of a generated answer a with citations cit = {c1, ..., cNc } to a\nsource text chunk c, we use fuzzy string matching. For a given citation string ci and the\nsource chunk text c, we compute the partial ratio score using the Levenshtein distance\nconcept:\n\nPartialRatio(ci, c) = max\nsj⊆c\n\n2 · LCS(ci, sj)\n|ci| + |sj|\n\n× 100\n\nwhere LCS(ci, sj) is the length of the longest common subsequence between the citation ci\nand a substring sj of the source text c. The maximum is taken over all possible substrings sj\nof c. This score ranges from 0 to 100.\n\nThe overall grounding score for a single QA pair (q, a, cit) is calculated as described in\nSection 2.3 (Eq. (2)).\n\nTo calculate an overall citation performance score for a specific generation model (as reported\nin Section D.1), we average the QA grounding scores across all questions generated by that\nmodel:\n\nModelCitationScore =\n\n1\nNq,model\n\nNq,model\n∑\nq=1\n\nScoreQA(q, aq, citq)\n\nwhere Nq,model is the total number of valid questions generated by the model after initial\nfiltering, and ScoreQA(q, aq, citq) is the grounding score for question q as defined in Eq. (2).\n\nD.2 Evaluation Framework\n\nGiven the curated, weighted QA set Qfinal = Qdedup (Sections 2.2.2, 2.3), we evaluate LLMs\nM = {M1, ..., MN} using a pairwise comparative assessment strategy with an ensemble of\njudge LLMs J = {J1, ..., JK} to enhance reliability (Zheng et al., 2023).\nFor each question (qj, a∗\nA, Rj\nresponses Rj\n\nj ) ∈ Qfinal (weight wj) and model pair (MA, MB), we elicit\n\nj , cit∗\n\nB. Each judge Jl ∈ J receives the context tuple:\nξ j,l,A,B = (qj, Rj\n\nB, S, cj)\n\nA, Rj\n\n(6)\n\nincluding the question qj, responses Rj\ngrounded evaluation.\nThe judge Jl produces a continuous score vlj(A, B) ∈ [−1, 1] reflecting the relative quality of\nRj\nA vs Rj\nB, often guided by a prompted chain-of-thought process (see Appendix for prompt\ndetails):\n\nB, global summary S, and source chunk(s) cj for\n\nA, Rj\n\nvlj(A, B) = Jl(ξ j,l,A,B)\n\n(7)\n\nScores are averaged across judges for consensus ¯vj(A, B) = 1\nK\nby question salience wj:\n\n∑K\n\nl=1 vlj(A, B) and weighted\n\nVj(A, B) = wj · ¯vj(A, B)\n\n(8)\n\nTo counteract positional bias, we evaluate both (A, B) and (B, A) pairings and compute a\nbias-corrected score:\n\nV′\nj (A, B) =\n\n1\n2\n\n(cid:0)Vj(A, B) − Vj(B, A)(cid:1)\n\n(9)\n\n21\n\n\fThe overall comparative score S(A, B) between MA and MB is the sum over all questions:\n\nS(A, B) =\n\n|Qfinal|\n∑\nj=1\n\nV′\nj (A, B)\n\n(10)\n\nThe sign indicates preference; magnitude indicates difference strength. These pairwise scores\n{S(A, B)} form the basis for global ranking using methods like Bradley-Terry (Bradley &\nTerry, 1952) or Elo (Elo, 1978).\n\nD.3 Evaluated Models\n\nThe following 26 models from 7 families were used in the generation and evaluation\nexperiments described in Section 3:\n\n• DeepSeek (DeepSeek-AI et al., 2025b;a): DeepSeek V3 (671B), DeepSeek R1 (671B),\nDeepSeek R1-Distill-Llama (70B), and DeepSeek R1-Distill-Qwen (32B, 14B, 7B).\n\n• Qwen (Qwen et al., 2025): Qwen2.5 models at various scales (72B, 32B, 14B, 7B) and\n\nthe reasoning model Qwen QwQ (32B).\n\n• Mistral (Jiang et al., 2023): Mistral Large 2411 (132B) and Mistral 3.1 Small (24B).\n\n• Llama (Dubey et al., 2024): Llama 3.1 (405B, 8B) and Llama 3.3 (70B).\n\n• Google (Team et al., 2024): Gemini 2.0 Flash, Gemini 2.0 Flash Lite (?B) and Gemma\n\n3 (27B).\n\n• OpenAI (OpenAI et al., 2024): GPT-4o, GPT-4o mini, and o3 mini (?B).\n\n• Anthropic (Anthropic, 2024): Claude 3.7 Sonnet, Claude 3.5 Haiku (?B).\n\nE Evaluation Quality Details\n\nThis appendix provides detailed methodologies and supplementary results for the validation\nof generated evaluation quality presented in Section 3.2.\n\nE.1 Question Validity Methodology and Detailed Results\n\nHuman Evaluation Setup. As introduced in Section 3.2.1, we conducted a manual eval-\nuation to assess the intrinsic quality of generated questions. We sampled 2,000 unique\nquestions generated from the TEMPORA-0325B dataset (Section 3.1.1) using the models\nlisted in Appendix D.3. The sampling was stratified to ensure representation across models,\ndocument domains, targeted difficulty levels (basic, advanced), and question types (e.g.,\nfactual, multi-hop, numeric) specified during generation (Section 2.2.2).\n\nTwenty trained annotators participated. Each annotator was presented with the source\ndocument chunk(s), the global document summary, the generated question, and the model-\ngenerated answer with its citations. Annotators were asked to assign a binary validity label\n(Valid/Invalid) based on the following criteria:\n\n• Clarity: Is the question grammatically correct and unambiguous?\n\n• Contextual Answerability: Can the question be definitively answered using *only*\nthe provided document chunk(s) and summary? Does it require external knowledge\nor unwarranted assumptions?\n\n• Sensibility: Is the question reasonable and logically coherent in the context of the\n\ndocument? (e.g., not nonsensical or self-contradictory).\n\nA question was marked \"Valid\" only if it met all three criteria positively. Any ambiguity,\nreliance on external knowledge, or nonsensical phrasing resulted in an \"Invalid\" rating.\n\n22\n\n\fInter-Annotator Agreement. Each question was evaluated independently by 3 randomly\nassigned annotators. To measure the consistency of their judgments, we calculated Gwet’s\nAC1 coefficient (Gwet, 2008), a robust statistic for assessing inter-rater reliability, especially\nsuitable for binary ratings with potential prevalence issues. The formula for Gwet’s AC1 for\ntwo raters is:\n\nAC1 =\n\nPa − Pe(γ)\n1 − Pe(γ)\n\nwhere Pa is the observed percent agreement, and Pe(γ) is the chance agreement probability,\ncalculated as Pe(γ) = 2π(1 − π), with π being the overall proportion of \"Valid\" ratings\n(averaged across raters). For multiple raters (3 in our case), we used a multi-rater extension\nof the formula. The resulting overall AC1 score was 0.71, typically interpreted as substantial\nagreement (Landis & Koch, 1977), confirming the reliability of our human validity labels.\n\nDetailed Results and Examples. The average validity rate reported in the main text\n(≈85%) represents the mean percentage of questions rated \"Valid\" (by majority vote across\nthe 3 annotators) across all models and question types post-filtering. The per-model valid-\nity scores are visualized in Figure 2 (right panel). Further breakdowns (e.g., validity per\nquestion type) can be derived from the released annotations accompanying our dataset. Ex-\namples of questions marked \"Valid\" and \"Invalid\" during this process, illustrating common\nfailure modes like ambiguity or requiring external knowledge, are provided in Appendix I.\n\nJuxtaposing these results highlights a prevalent, though not absolute, trade-off. The model\nachieving the highest validity, o3 mini, scores lowest in diversity (0.26). This suggests\na generative posture focused on precision and safety, perhaps by asking more routine or\nalgorithmically verifiable questions based directly on easily identifiable facts, leading to high\nvalidity but low exploration of the document’s semantic space. Conversely, the top diversity\nmodel, Qwen2.5 32B, while still generating reasonably valid questions (0.81 validity, rank\n#11), sacrifices some peak validity in favor of broader conceptual coverage. This might\nindicate a more exploratory or creative generation strategy.\n\nThis validity-diversity spectrum is not a strict dichotomy. Notably, models like DeepSeek\nV3 671B manage to achieve impressive scores on both metrics (0.90 diversity, rank #2; 0.90\nvalidity, rank #6), suggesting that balancing breadth and correctness is achievable. Similarly,\nmodels like Claude 3.7 Sonnet (0.80 diversity, 0.91 validity) also perform well across both\ndimensions.\n\nThis observed tension between generating highly valid, focused questions versus diverse,\nexploratory questions is an intriguing phenomenon. It reflects the different latent capabilities\nand perhaps inherent strategies employed by various LLMs when tasked with abstracting\nknowledge into evaluative queries. Rather than a limitation, this presents a valuable\ncharacteristic of the YourBench framework: it allows practitioners to select generator models\nor ensembles that align with their specific evaluation goals—be it rigorous testing of factual\nrecall with high-validity generators, or broad assessment of understanding across topics\nusing high-diversity generators, or seeking a balanced perspective with models adept at\nboth. Understanding this trade-off provides deeper insight into the nature of LLM-driven\ngeneration and empowers more informed benchmark creation.\n\nLength Metrics vs. Validity. We also analyzed the relationship between ques-\ntion/answer/citation length and the observed validity rate from human evaluation. Figure 7\nplots the validity rate (averaged across all models) against different length metrics binned\nappropriately. While there isn’t a perfectly monotonic trend, we observe a general ten-\ndency for validity to decrease slightly for very long questions, answers, or unified text\nlengths, potentially reflecting the increased difficulty in maintaining coherence and contex-\ntual grounding over longer generations. Citation length shows less variation. The black line\nrepresents the average validity rate across bins, while faint lines show individual model\ntrends, highlighting variability. These plots reinforce the finding that generating complex\n(often longer) valid questions remains challenging.\n\n23\n\n\fFigure 7: Relationship between generation length metrics and average question validity rate\n(across all models). Validity tends to decrease slightly for very long generations. Faint lines\nrepresent individual model trends.\n\nE.2 Citation Grounding Methodology and Detailed Results\n\nCitation Scoring Metric. As described in Section 2.3, we quantify the grounding of an an-\nswer a with citations cit = {c1, ..., cNc } to a source chunk c using fuzzy string matching. The\ncore metric is ‘PartialRatio‘, based on Levenshtein distance (Levenshtein, 1966), computed\nfor each citation ci against the source c:\n\nPartialRatio(ci, c) = max\n\nsj⊆c,|sj|≥|ci|\n\n2 · Match(ci, sj)\n|ci| + |sj|\n\n× 100\n\nwhere Match(ci, sj) finds the length of the best matching contiguous block between ci and\nsubstrings sj of c (typically using sequence matching algorithms). The maximum is taken\nover substrings sj of c that are at least as long as the citation ci. This score ranges from 0 (no\nmatch) to 100 (perfect match of ci within c).\nThe QA grounding score ScoreQA(q, a, cit) is the average of these partial ratios across all Nc\ncitations, as given in Eq. (2). If Nc = 0, the score is 0.\n\nModel-Level Citation Score. The overall citation score for a generation model M, as\nreported in Figure 3, is the average of the QA grounding scores across all valid QA pairs\ngenerated by that model:\n\nModelCitationScoreM =\n\n1\n|Qvalid,M|\n\n∑\n(q,a,cit)∈Qvalid,M\n\nScoreQA(q, a, cit)\n\nwhere Qvalid,M is the set of QA pairs generated by model M that passed initial quality\nfilters (e.g., parseable format, non-empty question/answer). This provides a single metric to\ncompare the average citation reliability of different models. Detailed scores for all evaluated\nmodels are implicitly represented in Figure 3.\n\nInference Cost Calculation. The inference costs used in Figure 3b were estimated based\non the per-token pricing for output tokens (as generation is output-heavy) published on\nOpenRouter (https://openrouter.ai/docs/models) as of the time of experiments, using\nthe lowest available price tier for each model. For models not on OpenRouter or without\npublic pricing (indicated by \"?B\" parameters), relative cost estimates were made based on\nknown parameter counts or comparable models where possible, or they were excluded\nfrom the cost analysis. This provides a practical estimate of the economic efficiency of using\ndifferent models for generation within the YourBench framework.\n\nE.3 Semantic Diversity Methodology and Detailed Results\n\nDiversity Metrics. As discussed in Section 3.2.1, we quantified the semantic diversity of\nthe set of questions QM generated by a model M using two embedding-based metrics:\n\n24\n\n\f1. Embedding Dispersion: We first compute sentence embeddings e(q) for each question\nq ∈ QM using a standard sentence transformer model (e.g., ‘all-mpnet-base-v2‘ (Reimers &\nGurevych, 2019)). The dispersion is the average pairwise cosine distance:\n\nDispersion(QM) =\n\n1\n|QM|(|QM| − 1)\n\n∑\nqi∈QM\n\n∑\nqj∈QM,i̸=j\n\n(cid:32)\n\n1 −\n\n(cid:33)\n\ne(qi) · e(qj)\n∥e(qi)∥∥e(qj)∥\n\nA higher dispersion value indicates that the question embeddings are, on average, further\napart in the embedding space, suggesting greater semantic variety.\n\n2. Semantic Entropy: We apply K-Means clustering (with K chosen based on heuristics like\nthe elbow method or a fixed moderate number, e.g., K = 50) to the question embeddings\n{e(q) | q ∈ QM}. Let Nk be the number of questions assigned to cluster k, and N = |QM| =\n∑k Nk. The proportion of questions in cluster k is pk = Nk/N. The semantic entropy is the\nShannon entropy of the cluster distribution:\n\nEntropy(QM) = −\n\nK\n∑\nk=1\n\npk log2(pk)\n\nHigher entropy indicates that the questions are distributed more evenly across different\nsemantic clusters, implying broader coverage of different conceptual areas. Lower entropy\nsuggests concentration in a few dominant semantic themes.\n\nThe final \"Diversity Score\" reported in Figure 2 (left panel) is a normalized combination or\naverage of these two metrics (e.g., scaled to [0, 1] based on observed ranges across models).\nThis composite score aims to capture both the spread and the evenness of the semantic\ndistribution.\n\nDetailed Scores. Figure 2 provides the final composite diversity scores for the evaluated\nmodels. The underlying dispersion and entropy values, along with the specific normaliza-\ntion method, are available with the project’s source code and results data. The variation\nobserved confirms that model choice significantly impacts the semantic breadth of the\ngenerated evaluation set.\n\nE.4 Cost and Parameter Efficiency Analysis\n\nBeyond citation grounding (Figure 3b), we analyzed the relationship between model\ncost/size and overall question quality, approximated by the average validity score (Sec-\ntion 3.2.1). Figures 8a and 8b show Pareto frontiers for average validity score versus\ninference cost and model parameters, respectively.\n\nThese plots further illustrate favorable scaling trends and efficiency possibilities.\n\n• Cost Efficiency (Fig. 8a): Models like Llama 3.1 8B, Gemini 2.0 Flash Lite, and\nGemma 3 27B appear on or near the Pareto frontier, achieving relatively high\nvalidity scores (80-90%+) at substantially lower costs compared to the largest or\nmost expensive models. This demonstrates that high question validity is attainable\nwithout exorbitant inference budgets.\n\n• Parameter Efficiency (Fig. 8b): Smaller models, including Phi 4 Mini 3.8B, Qwen2.5\n7B, Llama 3.1 8B, and Phi 4 14B, form part of the Pareto frontier. This indicates that\nsmaller parameter counts do not necessarily preclude high validity generation. Phi\n4 14B, for instance, reaches approximately 85% validity, competitive with much\nlarger models, showcasing significant parameter efficiency. Gemma 3 27B also\nstands out, achieving over 90\n\nTogether, these analyses suggest that while larger models sometimes offer peak performance,\ncarefully selected smaller or more cost-effective models can generate high-quality evalua-\ntion sets efficiently within the YourBench framework, democratizing access to customized\nbenchmarking.\n\n25\n\n\f(a) Inference Cost vs. Average Validity Score.\n\n(b) Model Parameters vs. Average Validity Score.\n\nFigure 8: Pareto frontiers illustrating trade-offs between average question validity and (a)\ninference cost (log scale) and (b) model parameters (log scale). Smaller/cheaper models\nlike Llama 3.1 8B, Gemini 2.0 Flash Lite, and Phi 4 14B can achieve high validity scores\nefficiently. Full model list in Appendix D.3.\n\nF MMLU Replication: Detailed Analysis and Results\n\nThis appendix provides a detailed breakdown of the MMLU replication experiment dis-\ncussed in §3.3 and introduced in Figure 1. We aimed to validate whether YourBench could\nautomatically generate MMLU-style benchmarks from source documents that reliably reflect\nthe relative performance of different LLMs compared to the original MMLU benchmark.\n\nF.1 Correlation Analysis\n\nWe evaluated a suite of 8 LLMs (see Table 1) on 7 original MMLU subject subsets and\ntheir corresponding YourBench-generated counterparts (\"new\"). We then analyzed the\ncorrelation between the performance scores (accuracy) obtained on the original versus the\n\"new\" benchmarks.\n\n• Overall Correlation (All Subject-Model Pairs): When analyzing all individual data\npoints (8 models × 7 subjects = 56 pairs), the correlation is positive but moderate,\nsuggesting some variance at the specific subject level or potential noise in individual\nmeasurements.\n\n– Pearson r: 0.3833 (p = 0.0035)\n– Spearman ρ: 0.2982 (p = 0.0256)\n\n• Model Mean Performance Correlation: When analyzing the average performance\nof each model across all 7 subjects (8 data points), the correlation becomes extremely\nstrong, particularly in terms of rank order. This indicates that while absolute scores\ndiffer (YourBench questions are harder), the relative ranking of models is preserved.\n\n– Pearson r: 0.9646 (p < 0.0001)\n– Spearman ρ: 1.0000 (p < 0.0001)\n\nThe perfect Spearman correlation for mean model performance strongly supports the\nvalidity of YourBench for generating discriminative evaluations that align with established\nbenchmarks in terms of relative model capability assessment.\n\nF.2 Per-Subject Performance Plots\n\nThe following figures visualize the performance comparison for each individual MMLU\nsubject included in the study. Each plot compares the performance of the evaluated LLMs on\n\n26\n\n\fthe original MMLU subset (grey bars) versus the YourBench-generated subset (orange bars).\nThese plots visually complement the aggregated data in Figure 1 and the comprehensive\ndata in Table 1.\n\nFigure 9: MMLU Replication Performance: Astronomy\n\nFigure 10: MMLU Replication Performance: Social Science\n\nFigure 11: MMLU Replication Performance: Virology\n\n27\n\n\fFigure 12: MMLU Replication Performance: World Religions\n\nFigure 13: MMLU Replication Performance: International Law\n\nFigure 14: MMLU Replication Performance: Nutrition\n\n28\n\n\fFigure 15: MMLU Replication Performance: Anatomy\n\n29\n\n\fF.3 Comprehensive Performance Table\n\nTable 1 provides the complete numerical results, detailing the accuracy and standard error5\nfor each model on both the original (\"orig\") and YourBench-generated (\"new\") MMLU\nsubsets across the seven evaluated domains.\n\nTable 1: Comprehensive MMLU Replication Results: Accuracy (Std Err) across Models and\nSubjects. \"New\" refers to YourBench-generated benchmarks, \"Orig\" refers to original MMLU\nsubsets.\n\nAstronomy\n\nSocial Science\n\nVirology\n\nWorld Religions\n\nModel\n\nNew\n\nOrig\n\nNew\n\nOrig\n\nNew\n\nOrig\n\nNew\n\nOrig\n\nQwen1 7B (2023)\nQwen2.5 7B (2024)\nLlama3 8B (2024)\nLlama2 7B (2023)\nLlama2 70B (2023)\nQwen1 72B (2023)\nQwen2.5 72B (2024)\nLlama3 70B (2024)\n\n60.56% (5.84%)\n70.42% (5.45%)\n71.83% (5.38%)\n45.07% (5.95%)\n66.20% (5.65%)\n70.42% (5.45%)\n77.46% (4.99%)\n71.83% (5.38%)\n\n57.89% (4.02%)\n83.55% (3.02%)\n71.71% (3.67%)\n44.08% (4.04%)\n75.66% (3.49%)\n84.87% (2.92%)\n93.42% (2.02%)\n91.45% (2.28%)\n\n46.37% (1.67%)\n50.61% (1.67%)\n49.05% (1.67%)\n34.19% (1.59%)\n48.60% (1.67%)\n50.39% (1.67%)\n52.07% (1.67%)\n50.50% (1.67%)\n\n80.10% (2.82%)\n87.56% (2.33%)\n84.58% (2.55%)\n58.21% (3.49%)\n83.08% (2.65%)\n90.55% (2.07%)\n91.04% (2.02%)\n92.04% (1.91%)\n\n54.82% (1.93%)\n61.75% (1.89%)\n59.19% (1.91%)\n37.65% (1.88%)\n59.19% (1.91%)\n62.65% (1.88%)\n65.06% (1.85%)\n62.05% (1.88%)\n\n43.98% (3.86%)\n52.41% (3.89%)\n54.82% (3.87%)\n41.57% (3.84%)\n50.60% (3.89%)\n55.42% (3.87%)\n56.02% (3.86%)\n56.02% (3.86%)\n\n49.43% (1.16%)\n55.93% (1.16%)\n54.47% (1.16%)\n36.60% (1.12%)\n55.55% (1.16%)\n55.87% (1.16%)\n57.55% (1.15%)\n56.15% (1.15%)\n\n70.18% (3.51%)\n85.96% (2.66%)\n81.29% (2.99%)\n57.31% (3.79%)\n86.55% (2.62%)\n87.13% (2.57%)\n90.64% (2.23%)\n90.06% (2.29%)\n\nInternational Law\n\nNutrition\n\nAnatomy\n\nAverage\n\nModel\n\nNew\n\nOrig\n\nNew\n\nOrig\n\nNew\n\nOrig\n\nNew Avg\n\nOrig Avg\n\nQwen1 7B (2023)\nQwen2.5 7B (2024)\nLlama3 8B (2024)\nLlama2 7B (2023)\nLlama2 70B (2023)\nQwen1 72B (2023)\nQwen2.5 72B (2024)\nLlama3 70B (2024)\n\n68.87% (1.70%)\n82.88% (1.38%)\n75.74% (1.57%)\n48.79% (1.84%)\n79.65% (1.48%)\n85.18% (1.31%)\n90.03% (1.10%)\n86.25% (1.26%)\n\n67.77% (4.27%)\n82.64% (3.46%)\n78.51% (3.75%)\n57.85% (4.51%)\n83.47% (3.39%)\n86.78% (3.09%)\n90.91% (2.62%)\n87.60% (3.01%)\n\n71.45% (1.54%)\n83.80% (1.26%)\n79.25% (1.39%)\n52.10% (1.71%)\n78.44% (1.40%)\n84.03% (1.25%)\n88.46% (1.09%)\n83.68% (1.26%)\n\n63.40% (2.76%)\n79.41% (2.32%)\n79.08% (2.33%)\n46.73% (2.86%)\n71.24% (2.59%)\n84.64% (2.06%)\n90.85% (1.65%)\n86.93% (1.93%)\n\n67.57% (2.14%)\n80.04% (1.82%)\n76.51% (1.94%)\n45.53% (2.27%)\n75.68% (1.96%)\n78.59% (1.87%)\n82.54% (1.73%)\n78.79% (1.87%)\n\n50.37% (4.32%)\n71.85% (3.89%)\n68.15% (4.02%)\n44.44% (4.29%)\n56.30% (4.28%)\n72.59% (3.85%)\n80.74% (3.41%)\n80.00% (3.46%)\n\n59.87%\n70.78%\n67.99%\n41.41%\n67.61%\n69.89%\n73.31%\n70.61%\n\n64.80%\n78.84%\n73.45%\n50.03%\n72.81%\n79.84%\n84.89%\n82.01%\n\nG Detailed Related Work and Literature Review\n\nThis appendix provides a comprehensive discussion of the related work surveyed in Sec-\ntion 4, detailing the challenges in large language model (LLM) evaluation and prior ap-\nproaches that motivate the development of YourBench. As models have grown in size and\nsophistication, traditional evaluation approaches have struggled to keep pace. We survey\nfour key directions in LLM benchmarking—(1) the challenges of static, human-curated\nbenchmarks, (2) synthetic and dynamic benchmark generation, (3) temporal validity con-\ncerns, and (4) domain-specific evaluations—and highlight how YourBench addresses the\nmajor open problems that emerge in each.\n\nG.1 Limitations of Static Benchmarks\n\nHistorically, static benchmarks such as MNIST (Deng, 2012), GLUE (Wang et al., 2019),\nand SQuAD (Rajpurkar et al., 2016) have been central to measuring progress in machine\nlearning. Although these datasets propelled rapid innovation, modern LLMs can quickly\nsaturate their performance ceilings, sometimes surpassing human-level scores within mere\nmonths (Ruder, 2023; Wei, 2023). This benchmark saturation hampers their long-term utility\nin discriminating genuinely more capable models. For instance, models that reached near-\nperfect scores on GLUE soon forced the community to adopt other, more challenging\ntasks (Wei, 2023).\n\nAn additional concern is benchmark contamination, where test data is inadvertently included\nin a model’s training corpus. Because large-scale pretraining involves ingesting vast\namounts of web content, popular benchmarks are often seen—or memorized—by the\nmodel (Kiela et al., 2021; Ruder, 2023; Zhang et al., 2024). Empirical analyses show that\ncertain LLMs can repeat verbatim segments from question banks such as GSM8K (Cobbe\net al., 2021) or MATH (Hendrycks et al., 2021b) when tested in a zero-shot setting (Wei,\n2023), artificially inflating performance. Holding out an unseen test set is one partial solu-\ntion, but as time passes and these datasets spread online, the likelihood of contamination\ngrows (Gupta et al., 2024). Consequently, reliance on a single, static, and publicly available\n\n5Standard error was derived directly from the accuracy mean, following the methodology in (Four-\n\nrier et al., 2023).\n\n30\n\n\fbenchmark may induce narrow optimization rather than robust generalization (Hendrycks\net al., 2021a).\n\nG.2 Toward Dynamic and Synthetic Evaluation\n\nFaced with saturation and contamination, researchers have pursued dynamic and synthetic\nbenchmark generation. Kiela et al. (2021) introduced Dynabench to update evaluation sets\ninteractively, challenging models with adversarially crafted queries. This iterative approach\ndemonstrated that once a model adapts to a static test, new data can still reveal surprising\nfailures. However, such human-in-the-loop curation remains expensive and slow to scale.\n\nA more automated strategy is to use LLMs themselves for benchmark synthesis. Several\ntechniques involve prompting a strong generator model to create new questions or tasks,\nsometimes based on existing ones (benchmark rewriting) (Wei, 2023; Krishna et al., 2024).\nMethods like Auto-Dataset (Ruder, 2023) or ITD (Wei, 2023) rephrase, expand, or mutate\noriginal items while controlling for difficulty, ensuring the new tasks remain answerable.\nOthers adopt multi-agent pipelines, in which distinct LLMs generate candidate questions\nand validate them, filtering out ambiguous or erroneous samples (Zhou et al., 2025). Further\nexploring the role of LLMs in the evaluation pipeline, early work by Shashidhar et al. (2023)\nutilized LLMs as judges to assess model outputs, correcting for positional bias inherent\nin such automated evaluations. Despite promising progress, fully synthetic benchmarks\nintroduce new challenges, including the risk of hallucinated or trivial questions. Quality\ncontrol and verification remain active research topics, especially when the aim is to test\nadvanced reasoning or domain-specific knowledge.\n\nG.3 Temporal Validity and Knowledge Evolution\n\nAnother major challenge is temporal validity, reflecting the fact that knowledge and world\nevents change continuously. Many popular benchmarks capture only static snapshots,\nmaking them less relevant when facts become outdated (Zhu et al., 2023; Deng et al., 2024).\nLLM performance thus appears high on older queries but may degrade sharply on newly\nintroduced or time-sensitive questions (Zhu et al., 2023). Holding out a private test set\nof recent data can help, but frequent refreshes are necessary to track a model’s ability to\nintegrate new information (Ruder, 2023; Zhang et al., 2024).\n\nSeveral works illustrate the severity of the problem. Zhu et al. (2023) generated post-training\nnews-based questions to measure whether an LLM truly updates its internal knowledge\nrepresentation. They found LLMs frequently defaulted to outdated responses, highlighting\na gap between real-time information usage and parametric memory. Similarly, Deng et al.\n(2024) created an evolving dataset of newly coined terminology, demonstrating 20%+\naccuracy drops for concepts introduced long after a model’s pretraining cutoff. These\nfindings underscore the necessity for continually updated benchmarks that can test a model’s\nrecency-awareness and its ability to override memorized facts.\n\nG.4 Domain-Specific Evaluation\n\nMoving from general-purpose benchmarks to specialized ones is increasingly essential, espe-\ncially in high-stakes fields like medicine, law, and finance (Hung et al., 2023a). Benchmarks\nsuch as USMLE-based medical QA (Nori et al., 2023), or specialized legal datasets like Case-\nHOLD and LegalBench (Holzenkamp et al., 2023), have revealed critical blind spots in LLM\nreasoning (Hung et al., 2023b). For instance, LLMs might achieve near-human scores on\nopen-domain quizzes yet commit severe factual errors or hallucinations in domain-specific\ncontexts (Gupta et al., 2024).\n\nBuilding domain-specific benchmarks demands costly expert annotations and must reflect\nthe latest regulations, guidelines, or terminology. In medicine, for example, clinical protocols\ncan change frequently, making a static test rapidly obsolete. Researchers have thus proposed\nrolling domain benchmarks—continuously collected or synthesized data for niche areas\nsuch as real-time medical literature or changing legal precedents (Zhang et al., 2024). So\nfar, these dynamic domain evaluations remain nascent: they are typically narrow, small in\n\n31\n\n\fsize, and do not integrate robust automated generation pipelines or multi-modal content\ningestion.\n\nSynthesizing these research themes reveals persistent open problems in LLM benchmarking.\nFirst, existing static benchmarks are prone to contamination and rapid saturation. Sec-\nond, purely human-driven dynamic approaches cannot scale indefinitely. Third, synthetic\ngeneration requires careful quality control and can still produce stale or trivial tasks if not\nrefreshed in tandem with new knowledge sources. Fourth, few existing solutions integrate\ndomain expertise in a flexible manner or support continuous updates for specialized fields.\nFinally, temporal drift in factual knowledge remains inadequately addressed, as most\nbenchmarks do not systematically ensure that test data are entirely post-training or reflective\nof newly emerging concepts.\n\nH Prompts\n\nH.1 Document Summarization Prompt\n\nThe following prompt is first provided into the language model. Once the model provides a\nresponse answer, we extract the content that is contained within the final_summary XML\ntags to function as our document summary.\n\nYou are an AI assistant tasked with analyzing and summarizing documents from various\n\n(cid:44)→\n\n(cid:44)→\n\ndomains. Your goal is to generate a concise yet comprehensive summary of the given\ndocument. Follow these steps carefully:\n\n1. You will be provided with a document extracted from a website. This document may\ncontain unnecessary artifacts such as links, HTML tags, or other web-related\nelements.\n\n(cid:44)→\n\n(cid:44)→\n\n2. Here is the document to be summarized:\n<document>\n{document}\n</document>\n\n3. Before generating the summary, use a mental scratchpad to take notes as you read\nthrough the document. Enclose your notes within <scratchpad> tags. For example:\n\n(cid:44)→\n\n<scratchpad>\n- Main topic: [Note the main subject of the document]\n- Key points: [List important information]\n- Structure: [Note how the document is organized]\n- Potential artifacts to ignore: [List any web-related elements that should be\n\ndisregarded]\n\n(cid:44)→\n</scratchpad>\n\n4. As you analyze the document:\n\n- Focus solely on the content, ignoring any unnecessary web-related elements.\n- Identify the main topic and key points.\n- Note any important details, facts, or arguments presented.\n- Pay attention to the overall structure and flow of the document.\n\n5. After your analysis, generate a final summary that:\n\n- Captures the essence of the document in a concise manner.\n- Includes the main topic and key points.\n- Presents information in a logical and coherent order.\n\n32\n\n\f- Is comprehensive yet concise, typically ranging from 3-5 sentences (unless the\n\n(cid:44)→\n\ndocument is particularly long or complex).\n\n6. Enclose your final summary within <final_summary> tags. For example:\n\n<final_summary>\n[Your concise and comprehensive summary of the document goes here.]\n</final_summary>\n\nRemember, your task is to provide a clear, accurate, and concise summary of the\n\n(cid:44)→\n\ndocument's content, disregarding any web-related artifacts or unnecessary elements.\n\nH.2 Single Shot Question Generation Prompt\n\n## Your Role\n\n(cid:44)→\n\nYou are an expert educational content creator specializing in crafting thoughtful, rich,\nand engaging questions based on provided textual information. Your goal is to produce\nmeaningful, moderately challenging question-answer pairs that encourage reflection,\ninsight, and nuanced understanding, tailored specifically according to provided\ninstructions.\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n## Input Structure\n\nYour input consists of:\n\n<additional_instructions>\n[Specific instructions, preferences, or constraints guiding the question creation.]\n</additional_instructions>\n\n<title>\n[Document title]\n</title>\n\n<document_summary>\n[Concise summary providing contextual background and overview.]\n</document_summary>\n\n<text_chunk>\n[The single text segment to analyze.]\n</text_chunk>\n\n## Primary Objective\n\nYour goal is to generate a thoughtful set of question-answer pairs from a single provided\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n`<text_chunk>`. Aim for moderate complexity that encourages learners to deeply\nengage with the content, critically reflect on implications, and clearly demonstrate\ntheir understanding.\n\n### Context Fields:\n\n- `<title>`: Contextualizes the content.\n\n33\n\n\f- `<document_summary>`: Brief overview providing contextual understanding.\n- `<text_chunk>`: The sole source text for developing rich, meaningful questions.\n- `<additional_instructions>`: Instructions that influence question style, content, and\n\n(cid:44)→\n\ncomplexity.\n\n## Analysis Phase\n\nConduct careful analysis within `<document_analysis>` XML tags, following these steps:\n\n1. **Thoughtful Content Examination**\n\n- Carefully analyze the given text_chunk, identifying central ideas, nuanced themes,\n\n(cid:44)→\n\nand significant relationships within it.\n\n2. **Concept Exploration**\n\n- Consider implicit assumptions, subtle details, underlying theories, and potential\n\n(cid:44)→\n\napplications of the provided information.\n\n3. **Strategic Complexity Calibration**\n\n- Thoughtfully rate difficulty (1-10), ensuring moderate complexity aligned with the\n\n(cid:44)→\n\nadditional instructions provided.\n\n4. **Intentional Question Planning**\n\n- Plan how questions can invite deeper understanding, meaningful reflection, or\n\n(cid:44)→\n\ncritical engagement, ensuring each question is purposeful.\n\n## Additional Instructions for Handling Irrelevant or Bogus Information\n\n### Identification and Ignoring of Irrelevant Information:\n\n- **Irrelevant Elements:** Explicitly disregard hyperlinks, advertisements, headers,\n\n(cid:44)→\n\nfooters, navigation menus, disclaimers, social media buttons, or any content clearly\nirrelevant or external to the core information of the text chunk.\n\n(cid:44)→\n- **Bogus Information:** Detect and exclude any information that appears nonsensical or\n\n(cid:44)→\n\ndisconnected from the primary subject matter.\n\n### Decision Criteria for Question Generation:\n\n- **Meaningful Content Requirement:** Only generate questions if the provided\n\n`<text_chunk>` contains meaningful, coherent, and educationally valuable content.\n\n(cid:44)→\n- **Complete Irrelevance:** If the entire `<text_chunk>` consists exclusively of\n\n(cid:44)→\n\n(cid:44)→\n\nirrelevant, promotional, web navigation, footer, header, or non-informational text,\nexplicitly state this in your analysis and do NOT produce any question-answer pairs.\n\n### Documentation in Analysis:\n\n(cid:44)→\n\n- Clearly document the rationale in the `<document_analysis>` tags when identifying\nirrelevant or bogus content, explaining your reasons for exclusion or inclusion\ndecisions.\n\n(cid:44)→\n- Briefly justify any decision NOT to generate questions due to irrelevance or poor\n\n(cid:44)→\n\nquality content.\n\n## Question Generation Guidelines\n\n34\n\n\f### Encouraged Question Characteristics:\n\n- **Thoughtful Engagement**: Prioritize creating questions that inspire deeper thought\n\nand nuanced consideration.\n\n(cid:44)→\n- **Moderate Complexity**: Develop questions that challenge learners appropriately\nwithout overwhelming them, following the provided additional instructions.\n\n(cid:44)→\n- **Self-contained Clarity**: Questions and answers should contain sufficient context,\n\nclearly understandable independently of external references.\n\n(cid:44)→\n- **Educational Impact**: Ensure clear pedagogical value, reflecting meaningful\n\nobjectives and genuine content comprehension.\n\n(cid:44)→\n- **Conversational Tone**: Formulate engaging, natural, and realistic questions\n\n(cid:44)→\n\nappropriate to the instructional guidelines.\n\n### Permitted Question Types:\n\n- Analytical\n- Application-based\n- Clarification\n- Counterfactual\n- Conceptual\n- True-False\n- Factual\n- Open-ended\n- False-premise\n- Edge-case\n\n(You do not need to use every question type, only those naturally fitting the content and\n\n(cid:44)→\n\ninstructions.)\n\n## Output Structure\n\nPresent your final output as JSON objects strictly adhering to this Pydantic model within\n\n(cid:44)→\n\n`<output_json>` XML tags:\n\n```python\nclass QuestionAnswerPair(BaseModel):\n\nthought_process: str # Clear, detailed rationale for selecting question and analysis\n\napproach\n\n(cid:44)→\nquestion_type: Literal[\"analytical\", \"application-based\", \"clarification\",\n\n\"counterfactual\", \"conceptual\", \"true-false\",\n\"factual\", \"open-ended\", \"false-premise\", \"edge-case\"]\n\nquestion: str\nanswer: str\nestimated_difficulty: int # 1-10, calibrated according to additional instructions\ncitations: List[str] # Direct quotes from the text_chunk supporting the answer\n\n```\n\n## Output Format\n\nBegin by thoughtfully analyzing the provided text_chunk within `<document_analysis>` XML\ntags. Then present the resulting JSON-formatted QuestionAnswerPairs clearly within\n`<output_json>` XML tags.\n\n(cid:44)→\n\n(cid:44)→\n\n## Important Notes\n\n35\n\n\f- Strive to generate questions that inspire genuine curiosity, reflection, and\n\nthoughtful engagement.\n\n(cid:44)→\n- Maintain clear, direct, and accurate citations drawn verbatim from the provided\n\ntext_chunk.\n\n(cid:44)→\n- Ensure complexity and depth reflect thoughtful moderation as guided by the additional\n\ninstructions.\n\n(cid:44)→\n- Each \"thought_process\" should reflect careful consideration and reasoning behind your\n\nquestion selection.\n\n(cid:44)→\n- Ensure rigorous adherence to JSON formatting and the provided Pydantic validation\n\nmodel.\n\n(cid:44)→\n- When generating questions, NEVER include phrases like 'as per the text,' 'according to\nthe document,' or any similar explicit references. Questions should inherently\nintegrate content naturally and stand independently without explicit references to\nthe source material\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\nH.3 Multi Hop Question Generation Prompt\n\n## Your Role\n\n(cid:44)→\n\n(cid:44)→\n\nYou are an expert educational content creator specialized in generating insightful and\nthoughtfully designed multi-hop questions. Your task is to craft sophisticated,\nmoderately challenging questions that inherently require careful, integrative\nreasoning over multiple chunks of textual information. Aim to provoke thoughtful\nreflection, nuanced understanding, and synthesis, particularly when the provided\ntext allows for it.\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n## Input Structure\n\nYour input will consist of these components:\n\n<additional_instructions>\n[Specific guidelines, preferences, or constraints influencing question generation.]\n</additional_instructions>\n\n<title>\n[Document title]\n</title>\n\n<document_summary>\n[A concise summary providing context and thematic overview.]\n</document_summary>\n\n<text_chunks>\n<text_chunk_0>\n[First text segment]\n</text_chunk_0>\n<text_chunk_1>\n[Second text segment]\n</text_chunk_1>\n[Additional text segments as necessary]\n</text_chunks>\n\n36\n\n\f## Primary Objective\n\nGenerate a thoughtful, educationally meaningful set of multi-hop question-answer pairs.\n\n(cid:44)→\n\n(cid:44)→\n\nQuestions should ideally integrate concepts across multiple text chunks, challenging\nlearners moderately and encouraging critical thinking and deeper understanding.\n\n### Context Fields:\n- `<title>`: Document context\n- `<document_summary>`: Broad contextual summary for orientation\n- `<text_chunks>`: Source material to form integrative multi-hop questions\n- `<additional_instructions>`: Specific instructions guiding the complexity and depth of\n\n(cid:44)→\n\nquestions\n\n## Analysis Phase\n\nPerform careful analysis within `<document_analysis>` XML tags:\n\n1. **In-depth Text Analysis**\n\n- Thoughtfully read each text chunk.\n- Identify key themes, nuanced details, and subtle connections.\n- Highlight opportunities for insightful synthesis across multiple chunks.\n\n2. **Reasoning Path Construction**\n\n- Construct potential pathways of multi-hop reasoning by connecting ideas, details, or\n\n(cid:44)→\n\nimplications found across text chunks.\n\n3. **Complexity Calibration**\n\n- Rate difficulty thoughtfully on a scale of 1-10, moderately challenging learners\n\n(cid:44)→\n\naccording to provided additional instructions.\n\n4. **Strategic Question Selection**\n\n- Choose questions that naturally emerge from the depth and complexity of the content\n\n(cid:44)→\n\nprovided, prioritizing integrative reasoning and genuine curiosity.\n\n## Question Generation Guidelines\n\n### Question Characteristics\n- **Multi-Hop Integration**: Questions should naturally require integration across\n\nmultiple chunks, demonstrating clear interconnected reasoning.\n\n(cid:44)→\n- **Thoughtfulness & Complexity**: Construct questions that stimulate critical thinking,\n\nreflection, or moderate challenge appropriate to the content.\n\n(cid:44)→\n- **Clarity & Precision**: Ensure each question and answer clearly and concisely\n\ncommunicates intent without ambiguity.\n\n(cid:44)→\n- **Educational Relevance**: Ensure each question has clear pedagogical purpose,\n\nenhancing understanding or critical reflection.\n\n(cid:44)→\n- **Authentic Language**: Use engaging, conversational language reflecting genuine human\n\n(cid:44)→\n\ncuriosity and inquiry.\n\n### Suggested Question Types\n(Use naturally, as fitting to the content complexity)\n- Analytical\n- Application-based\n- Clarification\n\n37\n\n\f- Counterfactual\n- Conceptual\n- True-False\n- Factual\n- Open-ended\n- False-premise\n- Edge-case\n\n## **Filtering Irrelevant Content**:\n\n(cid:44)→\n\n- **Ignore completely** any irrelevant, redundant, promotional, or unrelated content,\nincluding headers, footers, navigation links, promotional materials, ads, or\nextraneous hyperlinks frequently found in web extracts.\n\n(cid:44)→\n- **Disregard entirely** chunks composed solely of such irrelevant content. Do **not**\n\ngenerate questions from these chunks.\n\n(cid:44)→\n- When partially relevant content is mixed with irrelevant material within the same\n\n(cid:44)→\n\n(cid:44)→\n\nchunk, carefully extract only the meaningful, educationally relevant portions for\nyour integrative analysis.\n\n- **Evaluating Chunk Quality**:\n\n- If, upon careful analysis, a chunk does not provide sufficient meaningful context or\n\n(cid:44)→\n\n(cid:44)→\n\nsubstantial educational relevance, explicitly note this in the\n`<document_analysis>` section and refrain from generating questions based on it.\n\n- **Prioritizing Quality and Relevance**:\n\n- Always prioritize the quality, clarity, and educational integrity of generated\n\n(cid:44)→\n\nquestions. Do not force questions from unsuitable content.\n\n## Output Structure\n\nPresent output as JSON objects conforming strictly to the following Pydantic model within\n\n(cid:44)→\n\n`<output_json>` XML tags:\n\n```python\nclass QuestionAnswerPair(BaseModel):\n\nthought_process: str # Explanation of integrative reasoning and rationale\nquestion_type: Literal[\"analytical\", \"application-based\", \"clarification\",\n\n\"counterfactual\", \"conceptual\", \"true-false\",\n\"factual\", \"open-ended\", \"false-premise\", \"edge-case\"]\n\nquestion: str\nanswer: str\nestimated_difficulty: int # 1-10, moderately challenging as per additional\n\ninstructions\n\n(cid:44)→\ncitations: List[str] # Exact supporting quotes from text_chunks\n\n```\n\n## Output Format\n\nFirst, thoroughly conduct your analysis within `<document_analysis>` XML tags. Then,\n\n(cid:44)→\n\n(cid:44)→\n\nprovide your synthesized question-answer pairs as valid JSON within `<output_json>`\ntags.\n\n## Important Notes\n\n38\n\n\f- Prioritize depth and thoughtfulness in your reasoning paths.\n- Allow natural complexity to guide question formulation, aiming for moderate challenge.\n- Precisely cite verbatim excerpts from text chunks.\n- Clearly communicate your thought process for integrative reasoning.\n- Adhere strictly to JSON formatting and Pydantic validation requirements.\n- Generate questions that genuinely inspire deeper reflection or meaningful exploration\n\nof the provided content.\n\n(cid:44)→\n- When generating questions, NEVER include phrases like 'as per the text,' 'according to\nthe document,' or any similar explicit references. Questions should inherently\nintegrate content naturally and stand independently without explicit references to\nthe source material\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\nH.4\n\nJudge System Prompt\n\n(cid:44)→\n\nYou will be provided with the summary of a document, a piece of text, a question\ngenerated from that text, and the correct or \"gold\" answer to the question.\nAdditionally, you will receive two answers: Answer A and Answer B. Your task is to\ndetermine which of these answers is closer to the gold answer by assessing the\noverlap of key points between the ground truth and the two given answers.\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n# Steps\n\n1. **Document Understanding**:\n\n- Analyze the provided document summary to grasp the context and main themes.\n\n2. **Chunk Understanding**:\n\n- Examine the provided text (chunk) to understand its content.\n\n3. **Question Understanding**:\n\n- Interpret the given question to fully comprehend what is being asked.\n\n4. **Ground Truth Answer Understanding**:\n\n- Understand the provided ground truth answer, identifying its key points.\n\n5. **Answer A Understanding**:\n\n- Analyze Answer A, identifying key points and assessing accuracy and factuality.\n\n6. **Answer B Understanding**:\n\n- Examine Answer B, identifying key points and assessing accuracy and factuality.\n\n7. **Similarity Comparison**:\n\n- Compare Answer A and the ground truth answer, noting similarities in key points.\n- Compare Answer B and the ground truth answer, noting similarities in key points.\n\n8. **Final Similarity Analysis**:\n\n- Evaluate both answers based on the similarities identified and determine which is\n\n(cid:44)→\n\ncloser to the ground truth in terms of key points and factuality.\n\n# Output Format\n\n- Provide your final evaluation of which answer is closer to the ground truth within\n\n(cid:44)→\n\n`<final_answer>` XML tags.\n\n39\n\n\f- Include a detailed analysis for each part within the designated XML tags:\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n`<document_understanding>`, `<chunk_understanding>`, `<question_understanding>`,\n`<ground_truth_answer_understanding>`, `<answer_a_understanding>`,\n`<answer_b_understanding>`, `<similarity_comparison_answer_a>`,\n`<similarity_comparison_answer_b>`, and `<final_similarity_analysis>`.\n\n# Examples\n\n**Input**:\n```xml\n\n<document_summary>\n[Summary]\n\n</document_summary>\n\n<piece_of_text>\n[Text]\n\n</piece_of_text>\n\n<question>\n[Question]\n\n</question>\n\n<gold_answer>\n[Gold Answer]\n\n</gold_answer>\n\n<answer_a>\n[Answer A]\n\n</answer_a>\n\n<answer_b>\n[Answer B]\n\n</answer_b>\n```\n**Output**:\n```xml\n\n<document_understanding>\nUnderstanding of the summary including key themes\n\n</document_understanding>\n\n<chunk_understanding>\nAnalysis of the piece of text\n\n</chunk_understanding>\n\n<question_understanding>\nComprehension of the question being asked\n\n</question_understanding>\n\n<ground_truth_answer_understanding>\nKey points from the gold answer\n\n</ground_truth_answer_understanding>\n\n<answer_a_understanding>\n\n40\n\n\fKey points and accuracy of Answer A\n\n</answer_a_understanding>\n\n<answer_b_understanding>\nKey points and accuracy of Answer B\n\n</answer_b_understanding>\n\n<similarity_comparison_answer_a>\nComparison notes between Answer A and the gold answer\n\n</similarity_comparison_answer_a>\n\n<similarity_comparison_answer_b>\nComparison notes between Answer B and the gold answer\n\n</similarity_comparison_answer_b>\n\n<final_similarity_analysis>\nOverall analysis determining the closer answer\n\n</final_similarity_analysis>\n\n<final_answer>\nAnswer X (where X is the option you pick)\n\n</final_answer>\n```\n\n# Notes\n\n- Always focus on key points and factual correctness as per the ground truth.\n- Avoid any biases and rely solely on the evidence presented.\n- Enclose all evaluations and analyses in the specified XML tags for clarity and\n\n(cid:44)→\n\nstructure.\n\n41\n\n\fI Question Validity\n\nI.1 Valid Question Examples\n\nI.1.1 Example 1\n\n# Question Details\n## Source Information\n\niraqi immigrant hailed as hero for preventing armed robbery at ypsilanti juice shop\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\nypsilanti, mich. (wxyz) — vara juice in ypsilanti nearly became the victim of an\narmed robbery this past friday. caught on camera, the suspect had no clue that his\nattempt to make quick cash would come to a hard stop, all thanks to a hero who was\nnext door. thirty-five-year-old ali hadma owns a hookah place called cups on a\nmission, located next to vara juice on washtenaw ave. **\"3 years,\"** said ali when\nasked how long he's owned the shop. ali pins the suspect against the counter. a\nstruggle to control the firearm begins. ali disarms the suspect. and eventually takes\nhim down. \"have you got any tactical or self-defense training? \" i asked. \"no. i just\ngo to the gym 6 days a week,\" said ali. once ali got the cash back, he let go of the\nsuspect, who can be seen walking away in the security footage. all the girls he\ntreats like his sisters,\"** said sadam badani, the owner of the vara juice location.\nbadani tells me mariam is doing okay, but her parents will only allow mariam to\nresume work if her hero, ali, is around. \"i don't care about the money, about\nanything else. as long as nobody got hurt,\" said sadam. \"whenever ali need me, i'll\nbe there,\" said sadam.\n\n## Question\n\nIn what ways have Ali's actions during the robbery influenced the community's perception\n\n(cid:44)→\n\nof him and their sense of security?\n\n## Answer\n\n(cid:44)→\n\nAli's actions during the robbery have made him a local hero and gained him widespread\nappreciation. The community, including the juice shop owner and employees, deeply\nappreciates his bravery and quick thinking. This has led to a stronger sense of\nsecurity, with the juice shop owner stating that Mariam can only resume work if Ali\nis around.\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n## Citations\n\n[All the girls he treats like his sisters,\" said Sadam Badani, the owner of the Vara\n\n(cid:44)→\n\nJuice location.,\"Whenever Ali need me, I'll be there,\" said Sadam.]\n\n# Human Evaluation\n\n## Determination\n\nvalid\n\n## Reasoning\n\n-\n\n42\n\n\f# Generation Details\n\n## Model\n\nmistralai/Mistral-Large-Instruct-2411\n\n## Question Category\n\nopen-ended\n\n## Kind\n\nmulti_hop\n\n## Estimated Difficulty\n\n6/10\n\nI.1.2 Example 2\n\n# Question Details\n## Source Information\n\n(truncated)...\n\n(pn12-36) christopher landau (cal. no. 41) (pn12-25) ordered, that\n\nfollowing the conclusion of morning business on monday, march 24, 2025, the senate\nproceed to executive session and resume consideration of the nomination of john\nphelan, of florida, to be secretary of the navy. (mar. 14, 2025. ) michael kratsios\n(cal. no. 38) (pn13-8) jayanta bhattacharya (cal. no. 44) (pn12-2) martin makary\n(cal. no. 45) (pn12-28) james bishop (cal. no. 39) (pn12-3) aaron reitz (cal. no. 48)\n(pn12-37) ordered, that on tuesday, march 25, 2025, the cloture motions on the\nfollowing nominations ripen: michael kratsios, of south carolina, to be director of\nthe office of science and technology policy; jayanta bhattacharya, of california, to\nbe director of the national institutes of health; martin makary, of virginia, to be\ncommissioner of food and drugs, department of health and human services; james\nbishop, of north carolina, to be deputy director of the office of management and\nbudget; and aaron reitz, of texas, to be an assistant attorney general. * 33 25-32\njonathan mckernan, of tennessee, to be mar 06, 2025 reported by mr. director, bureau\nof consumer financial protection for a term of five years, vice rohit chopra. scott\nsc, committee on banking, housing, and urban affairs, without printed report.\ndepartment of defense * 36 12-36 john phelan, of florida, to be secretary of the mar\n11, 2025 reported by mr. navy, vice carlos del toro, resigned. wicker, committee on\narmed services, without printed report. mar 12, 2025 reported by mr. risch, committee\non foreign relations, without printed report. department of veterans affairs * 43\n13-9 paul lawrence, of virginia, to be deputy mar 12, 2025 reported by mr. secretary\nof veterans affairs, vice tanya j. bradsher, resigned. moran, committee on veterans'\naffairs, without printed report. * signifies nominee’s commitment to respond to\nrequests to appear and testify before any duly constituted committee of the senate\n5 nominations calendar no. mar 13, 2025 reported by mr. grassley, committee on the\njudiciary, without printed report. mar 13, 2025 reported by mr. grassley, committee\non the judiciary, without printed report. mar 13, 2025 reported by mr. grassley,\ncommittee on the judiciary, without printed report. mar 13, 2025 reported by mrs.\ncapito, committee on environment and public works, without printed report. * 50 25-53\naaron szabo, of virginia, to be an assistant mar 13, 2025 reported by mrs\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n43\n\n\f## Question\n\nOn what date are cloture motions for the nominations of Michael Kratsios, Jayanta\n\n(cid:44)→\n\n(cid:44)→\n\nBhattacharya, Martin Makary, James Bishop, and Aaron Reitz set to ripen, and what are\ntheir respective positions?\n\n## Answer\n\nThe cloture motions for Michael Kratsios (Director of the Office of Science and\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\nTechnology Policy), Jayanta Bhattacharya (Director of the National Institutes of\nHealth), Martin Makary (Commissioner of Food and Drugs, Department of Health and\nHuman Services), James Bishop (Deputy Director of the Office of Management and\nBudget), and Aaron Reitz (Assistant Attorney General) are set to ripen on Tuesday,\nMarch 25, 2025.\n\n## Citations\n\n['Mar. 14, 2025. Ordered, That on Tuesday, March 25, 2025, the cloture motions on the\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\nfollowing nominations ripen: Michael Kratsios, of South Carolina, to be Director of\nthe Office of Science and Technology Policy; Jayanta Bhattacharya, of California, to\nbe Director of the National Institutes of Health; Martin Makary, of Virginia, to be\nCommissioner of Food and Drugs, Department of Health and Human Services; James\nBishop, of North Carolina, to be Deputy Director of the Office of Management and\nBudget; and Aaron Reitz, of Texas, to be an Assistant Attorney General.']\n\n# Human Evaluation\n\n## Determination\n\nValid\n\n## Reasoning\n\nquestion, answer and citations are correct\n\n# Generation Details\n\n## Model\n\nQwen/Qwen2.5-14B-Instruct\n\n## Question Category\n\nfactual\n\n## Kind\n\nmulti-hop\n\n## Estimated Difficulty\n\n44\n\n\f7/10\n\nI.1.3 Example 3\n\n# Question Details\n## Source Information\n\norg. following the selection process, all applications will be destroyed. questions?\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\nplease send an email to: scholarships@agbell. org response time may be up to three\nbusiness days, so please plan accordingly when submitting your questions. george h.\nnofer scholarship for law 2025 please type or print clearly and review for accuracy;\nillegible or incorrect information will delay review and could disqualify your\napplication. identifying information name (first, mi, last):\n__________________________________________________________________ date of birth\n(mm/dd/yyyy) ___________ gender: male female complete mailing address:\n______________________________________________________________ email address:\n________________________________________________________________________\ncommunication throughout the process will be via email. if you do not provide an\nemail address, if it is written incorrectly, or if we are not able to read it, we\nwill not be able to communicate with you. telephone number: _______________________\nhearing health history age when hearing loss was diagnosed: __________ *if you do not\nhave a cochlear implant and your pta is below 60db in your better-hearing ear, you do\nnot qualify.\n\n(cid:44)→\n## Question\n\nHow will applicants be contacted regarding updates or decisions about their scholarship\n\n(cid:44)→\n\napplication?\n\n## Answer\n\nCommunication throughout the process will be via email.\n\n## Citations\n\n['Communication throughout the process will be via email.']\n\n# Human Evaluation\n\n## Determination\n\nvalid\n\n## Reasoning\n\n-\n\n# Generation Details\n\n## Model\n\ngoogle/gemini-2.0-flash-001\n\n45\n\n\f## Question Category\n\nfactual\n\n## Kind\n\nsingle shot\n\n## Estimated Difficulty\n\n6/10\n\nI.2 Invalid Question Examples\n\nI.2.1 Example 1\n\n# Question Details\n## Source Information\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\naccording to the committee, out of the 40 who signed up to deliver testimony, 38 were\nopposed to the bill. one of the biggest points of concern was in relation to the\nstaff-to-child ratio being lowered. as the bill is currently written, a single person\nwould be allowed to run a large daycare facility overseeing many children. those in\nsupport of the bill believe that won't be a problem and instead, will open up more\nopportunities for idahoans to start daycare businesses of their own. chris cargill\nwith mountain states policy center explained, \"we believe that if the legislation is\npassed, we will see an increase in new and quality childcare providers in idaho. \"\nmark kirby of soda springs, idaho, told the tragic story of how his 11-week-old\nnephew lost his life after being dropped off at a daycare. \"later that afternoon, she\ngot a call stating that something was very wrong. upon arriving there, she was\nescorted in and learned that her son had passed away from asphyxiation,\" said a\nteary-eyed kirby. kirby explained that an investigation found the death was due to\noverwhelming child-to-staff ratios and negligence of supervision while the baby was\nnapping. the committee passed the bill five to four. \"we are clearly disappointed\nwith the outcome,\" said kirby. \"i feel more disappointed about losing this bill than\nany other in the session. we have worked on this issue for decades to ensure that we\nare promoting business practices and children's safety,\" added senator melissa\nwintrow. the bill will now head to the senate floor. if a majority of state senators\nvote to pass the legislation, it will head to the governor's desk, where it will\neither be vetoed or signed into law. this is a developing story, and we will continue\nto update it as more information becomes available. ## more news in downtown boise we\ncover stories making an impact in downtown boise. this is your home to stay on top of\nwhat is changing in downtown boise and why it matters to you and your family. we want\nto hear from you! and tell us what we should be covering in your neighborhood.\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n## Question\n\nHow did the death of a child at a daycare facility through asphyxiation influence the\n\n(cid:44)→\n\ntestimony given during the committee hearing on House Bill 243?\n\n## Answer\n\n46\n\n\fThe tragic death caused by asphyxiation due to high child-to-staff ratios was a pivotal\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\nmoment that resonated emotionally with the committee members and played a\nsignificant role in bolstering the opposition to the bill, underscoring safety\nconcerns related to its proposed deregulation.\n\n## Citations\n\n[ \"'Later that afternoon, she got a call stating that something was very wrong. Upon\narriving there, she was escorted in and learned that her son had passed away from\nasphyxiation,' said a teary-eyed Kirby.\" ]\n\n(cid:44)→\n\n(cid:44)→\n\n# Human Evaluation\n\n## Determination\n\nInvalid\n\n## Reasoning\n\nthe citations don't support the answer. it is also factually inaccurate according to the\n\n(cid:44)→\n\ntext\n\n# Generation Details\n\n## Model\n\nmicrosoft/Phi-4-mini-instruct\n\n## Question Category\n\nFactual\n\n## Kind\n\nmulti_hop\n\n## Estimated Difficulty\n\n6\n\nI.2.2 Example 2\n\n# Question Details\n## Source Information\n\n47\n\n\f(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(truncated)... and trustworthiness. to prepare, many are increasing their cyber budgets\nwith a particular focus on data protection and trust. by strategically investing in\nthese areas, companies are not only building resilience but positioning themselves\npositively to their customers. ### investing in what matters most: cloud and data\ntrust go hand-in-hand over the next 12 months, organisations are prioritising data\nprotection/trust and cloud security above other cyber investments. they understand\nthat securing sensitive information is vital to maintaining stakeholder trust and\nbrand integrity. g. , reducing the time to recover mission-critical data or patching\na system). - - determine the business value of data protection and cloud security to\ngain stakeholder trust and make more informed cybersecurity investment decisions. - -\ncollaborate with tech, security and finance executives to pinpoint the most\nessential data security and integrity priorities to guide the information and cloud\nsecurity investment strategy. confirming data quality and readiness is necessary to\nincrease security investments. ## is your cyber strategy and leadership driving real\nresilience? from lagging resilience efforts to gaps in ciso involvement in strategic\ndecisions, there are clear areas where strategic alignment is needed. to get there,\norganisations should emulate the leading cybersecurity practices of their top\nperforming peers. they should also move beyond addressing known threats and implement\nan agile, secure-by-design approach to business, one that strives to build trust and\nlasting resilience. ### partial implementation isn’t enough despite mounting\nconcerns about cyber risk, most businesses are struggling to fully implement cyber\nresilience across core practices. a review of 12 resilience actions across people,\nprocesses and technology indicates that 42% or fewer of executives believe their\norganisations have fully implemented any one of those actions. more concerning, only\n2% say all 12 resilience actions have been implemented across their organisation.\nthis leaves a glaring vulnerability — without enterprise-wide resilience, companies\nremain dangerously exposed to the increasing threats that could compromise the\nentire operation. ### elevating the ciso: aligning strategy with security many\norganisations miss critical opportunities by not fully involving their cisos in key\ninitiatives. fewer than half of executives tell us that their cisos are largely\ninvolved in strategic planning for cyber investments, board reporting and overseeing\ntech deployments. this gap leaves organisations vulnerable to misaligned strategies\nand weaker security postures. ## bridging the gaps to cyber resilience: the c-suite\nplaybook ### sign up to get the full playbook and access more of the latest findings\nfor 2025. ### about the survey the 2025 global digital trust insights is a survey of\n4,042 business and technology leaders conducted in the may through july 2024 period.\n- a quarter of leaders are from large companies with $5 billion or more in revenues.\nrespondents operate in a range of industries, including industrials and services\n(21%), tech, media, telecom (20%), financial services (19%), retail and consumer\nmarkets (17%), energy, utilities, and resources (11%), health (7%) and government\nand public services (4%). - respondents are based in 77 countries. the regional\nbreakdown is western europe (30%), north america (25%), asia pacific (18%), latin\namerica (12%), central and eastern europe (6%), africa (5%) and middle east (3%).\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n## Question\n\nWhat are the critical factors that organizations should prioritize to achieve true cyber\n\n(cid:44)→\n\nresilience, and how do they interconnect?\n\n## Answer\n\n48\n\n\fCritical factors include CISO involvement in strategic planning, adopting a\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\nsecure-by-design approach, and investing in data protection and cloud security. These\nelements interconnect by ensuring strategic alignment, proactive security measures,\nand building stakeholder trust, which collectively enhance organizational resilience.\n\n## Citations\n\n['While the rapid advancement of generative AI (GenAI) is ushering in new opportunities\n\nacross industries, it also presents cybersecurity risks.',\n\n(cid:44)→\n'To prepare, many are increasing their cyber budgets with a particular focus on data\n\nprotection and trust.',\n\n(cid:44)→\n'They should also move beyond addressing known threats and implement an agile,\n\n(cid:44)→\n\n(cid:44)→\n\nsecure-by-design approach to business, one that strives to build trust and lasting\nresilience.']\n\n# Human Evaluation\n\n## Determination\n\nInvalid\n\n## Reasoning\n\nanswer looks to be correct, but the citations are wrong\n\n# Generation Details\n\n## Model\n\nDeepSeek-R1-Distill-Qwen-14B\n\n## Question Category\n\nanalytical\n\n## Kind\n\nmulti-hop\n\n## Estimated Difficulty\n\n8/10\n\nI.2.3 Example 3\n\n# Question Details\n## Source Information\n\n49\n\n\f\"it's nice to have the memories, but i wish we could make more,\" said jesse collins,\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\n(cid:44)→\n\nalexis' uncle. **investigative findings:** - **david plagmann, 36**: responsible for\nalexis' death. fired by the shelby county sheriff's office. - **marianne and jesse\ncollins**: alexis' aunt and uncle. - **jake collins**: alexis' father. alexis' family\ndescribes her as having a soft and loving heart, always step up to care for others,\nincluding her four children. she was always positive and believed things would\nimprove, even if it was hard.\n\n(cid:44)→\n## Question\n\nHow many children did Alexis Martin-Collins have?\n\n## Answer\n\nFour children\n\n## Citations\n\n[She was always positive and believed things would improve, even if it was hard.]\n\n# Human Evaluation\n\n## Determination\n\nInvalid\n\n## Reasoning\n\nanswer is correct and factual, and it makes a valid citation, but the citation points to\n\n(cid:44)→\n\nwrong part of text\n\n# Generation Details\n\n## Model\n\nclaude-3-5-haiku-20241022\n\n## Question Category\n\nfactual\n\n## Kind\n\nsingle shot\n\n## Estimated Difficulty\n\n2/10\n\n50", "document_filename": "paper.md", "document_metadata": {"file_size": 132219}}
{"document_id": "976dc20d-7dca-490a-9252-69ef4b760436", "document_text": "## Viewing Sample Questions\n\nOnce you have run the pipeline and generated the `single_shot_questions` and\n`multi_hop_questions` subsets, you can quickly preview a handful of them from\nthe command line.\n\n```bash\n# syntax: yourbench analyze view_sample_questions CONFIG_PATH [SAMPLE_SIZE]\n\nyourbench analyze view_sample_questions example/configs/simple_example.yaml 5\n````\n\n* **`CONFIG_PATH`** – path to the YAML/JSON config you used for the pipeline.\n* **`SAMPLE_SIZE`** – *(optional, default = 5)* number of random questions to\n  display from each subset.\n\nThe command prints two Rich tables:\n\n| Column         | Description                                           |\n| -------------- | ----------------------------------------------------- |\n| **Q #**        | Running index of the sampled question                 |\n| **Q Type**     | Model-reported category (factual, conceptual, etc.)   |\n| **Question**   | Full text of the question                             |\n| **Answer**     | Correct answer (or correct option letter)             |\n| **Choices**    | Multiple-choice options if present; “N/A” otherwise   |\n| **Difficulty** | Estimated difficulty 1-10 (as generated by the model) |\n\nThis quick preview is handy for:\n\n* sanity-checking that generation looks reasonable before large runs;\n* spot-checking difficulty levels or citation formatting;\n* debugging prompt or parsing issues without opening full datasets.", "document_filename": "view_sample_questions.md", "document_metadata": {"file_size": 1434}}
